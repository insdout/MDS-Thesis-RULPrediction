{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9af604ef-ffc1-4420-867e-1d89cd0bc49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from preprocess import CMAPSSSlidingWin\n",
    "from train import TrainUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a504aa2e-c7d2-4daa-b029-54e6c1cdf9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f8d1b72-1434-4a96-a4b4-06ec420af517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMHCAtn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMHCAtn, self).__init__()\n",
    "        self.lstm = nn.LSTM(batch_first=True, input_size=17, hidden_size=50, num_layers=1)\n",
    "        self.attenion = Attention3dBlock()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=1500, out_features=50),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=50, out_features=10),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.handcrafted = nn.Sequential(\n",
    "            nn.Linear(in_features=34, out_features=10),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(in_features=20, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, handcrafted_feature):\n",
    "        y = self.handcrafted(handcrafted_feature)\n",
    "        x, (hn, cn) = self.lstm(inputs)\n",
    "        x = self.attenion(x)\n",
    "        # flatten\n",
    "        x = x.reshape(-1, 1500)\n",
    "        x = self.linear(x)\n",
    "        out = torch.concat((x, y), dim=1)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Attention3dBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention3dBlock, self).__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=30, out_features=30),\n",
    "            nn.Softmax(dim=2),\n",
    "        )\n",
    "\n",
    "    # inputs: batch size * window size(time step) * lstm output dims\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.permute(0, 2, 1)\n",
    "        x = self.linear(x)\n",
    "        x_probs = x.permute(0, 2, 1)\n",
    "        # print(torch.sum(x_probs.item()))\n",
    "        output = x_probs * inputs\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89ac799-8fb3-40eb-9c19-85cabd13af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBaseline(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden=64, n_layers=2):\n",
    "        super(LSTMBaseline, self).__init__()\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_hidden,\n",
    "            batch_first=True,\n",
    "            num_layers=n_layers,\n",
    " \n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=30*self.n_hidden, out_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=8, out_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=8, out_features=1)\n",
    "        )\n",
    "\n",
    "        self.printed = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, (hidden, _) = self.lstm(x)\n",
    "        if not self.printed:\n",
    "            print(\"output shape:\", list(lstm_output.shape), \"hidden shape:\", list(hidden.shape))\n",
    "            self.printed=True\n",
    "        lstm_out = lstm_output.reshape(-1, self.n_hidden*30)  # output last hidden state output\n",
    "        y_pred = self.linear(lstm_out)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaadcf8d-6950-427a-8325-5e6ec3d9b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, time_steps=30):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(time_steps, time_steps),\n",
    "            nn.Softmax(dim=2),\n",
    "            ##nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # inputs: batch size * window size(time step) * lstm output dims\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.permute(0, 2, 1)\n",
    "        x = self.linear(x)\n",
    "        x_probs = x.permute(0, 2, 1)\n",
    "        #print(\"probs\")\n",
    "        #print(x_probs)\n",
    "        #print()\n",
    "        # print(torch.sum(x_probs.item()))\n",
    "        output = x_probs * inputs\n",
    "        return output\n",
    "\n",
    "    \n",
    "class LSTMBaselineAtn(nn.Module):\n",
    "    def __init__(self, n_features, time_steps=30, n_hidden=64, n_layers=2):\n",
    "        super(LSTMBaselineAtn, self).__init__()\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.time_steps = time_steps\n",
    "        self.n_features = n_features\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_hidden,\n",
    "            batch_first=True,\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        self.attention = AttentionBlock(time_steps)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=self.time_steps*self.n_hidden, out_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=8, out_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=8, out_features=1)\n",
    "        )\n",
    "        self.printed = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, (hidden, _) = self.lstm(x)\n",
    "        if not self.printed:\n",
    "            print(\"output shape:\", *lstm_output.shape, \"hidden shape:\", *hidden.shape)\n",
    "            self.printed=True\n",
    "            print()\n",
    "        x = self.attention(lstm_output)\n",
    "        #print(x)\n",
    "        #print()\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1, self.time_steps*self.n_hidden)\n",
    "        #print(\"x shape\", x.shape)\n",
    "        lstm_out = hidden[-1]  # output last hidden state output\n",
    "        y_pred = self.linear(x)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2f4355-218c-4ec3-87c1-868aab5c0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMStackedHiddenStates(nn.Module):\n",
    "    def __init__(self, input_size=17, time_steps=30, hidden_size=64, num_layers=4):\n",
    "        super(LSTMStackedHiddenStates, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.time_steps = time_steps\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstms = self.get_lstm()\n",
    "        self.fc =  nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=16, out_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=8, out_features=1)\n",
    "        )\n",
    "    \n",
    "    def get_lstm(self):\n",
    "        lstms = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            input_size = self.input_size if i == 0 else self.hidden_size\n",
    "            lstms.append(nn.Sequential(nn.LSTM(input_size=input_size, hidden_size=self.hidden_size, batch_first=True)))\n",
    "        return lstms\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        #print(\"input shape:\", inputs.shape)\n",
    "        x = inputs\n",
    "        hidden_states = []\n",
    "        for layer in self.lstms:\n",
    "            x, (hn, cn) = layer(x)\n",
    "            hidden_states.append(hn)\n",
    "        \n",
    "        h_1 = hidden_states[1]\n",
    "        h_1 = h_1.view(-1, self.hidden_size)\n",
    "        h_o = hidden_states[-1]\n",
    "        h_o = h_o.view(-1, self.hidden_size)\n",
    "        #print(\"h_1 shape:\", h_1.shape)\n",
    "        out = self.fc(h_1+h_o)\n",
    "        #print(\"out shape:\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2b0e8b1-ab72-4b44-8857-82b26c27439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMStackedHiddenStates2(nn.Module):\n",
    "    def __init__(self, input_size=17, time_steps=30, hidden_size=64, num_layers=4):\n",
    "        super(LSTMStackedHiddenStates2, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.time_steps = time_steps\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstms = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=self.hidden_size, \n",
    "                             num_layers=self.num_layers, \n",
    "                             batch_first=True, \n",
    "                             dropout=0.1)\n",
    "        \n",
    "        self.fc =  nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=16, out_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=8, out_features=1)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        #print(\"input shape:\", inputs.shape)\n",
    "        _, (hidden_states, c_n) = self.lstms(inputs)\n",
    "        \n",
    "        h_1 = hidden_states[1]\n",
    "        h_1 = h_1.view(-1, self.hidden_size)\n",
    "        h_o = hidden_states[-1]\n",
    "        h_o = h_o.view(-1, self.hidden_size)\n",
    "        #print(\"h_1 shape:\", h_1.shape)\n",
    "        out = self.fc(h_1+h_o)\n",
    "        #print(\"out shape:\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e32b08-927c-4290-99df-80fef7937c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class LSTM1(nn.Module):\n",
    "    \"\"\"LSTM architecture\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, seq_length=1):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.input_size = input_size  # input size\n",
    "        self.hidden_size = hidden_size  # hidden state\n",
    "        self.num_layers = num_layers  # number of layers\n",
    "        self.seq_length = seq_length  # sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True,\n",
    "                            dropout=0.1)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hidden_size, 16)  # fully connected 1\n",
    "        self.fc_2 = nn.Linear(16, 8)  # fully connected 2\n",
    "        self.fc = nn.Linear(8, 1)  # fully connected last layer\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: input features\n",
    "        :return: prediction results\n",
    "        \"\"\"\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(\"cuda\"))  # hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(\"cuda\"))  # internal state\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))  # lstm with input, hidden, and internal state\n",
    "\n",
    "        hn_o = torch.Tensor(hn.detach().cpu().numpy()[-1, :, :])\n",
    "        hn_o = hn_o.view(-1, self.hidden_size).to(\"cuda\")\n",
    "        hn_1 = torch.Tensor(hn.detach().cpu().numpy()[1, :, :])\n",
    "        hn_1 = hn_1.view(-1, self.hidden_size).to(\"cuda\")\n",
    "\n",
    "        out = self.relu(self.fc_1(self.relu(hn_o + hn_1)))\n",
    "        out = self.relu(self.fc_2(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be819b00-1ba2-4d95-9293-f5e8e7e4bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    print(\"V shape:\", v.shape)\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    print(\"Attention logits shape: \", attn_logits.shape)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    print(\"Len Attention logits summ dim 1: \", len(attention.sum(dim=1)[0]))\n",
    "    print(\"Attention logits summ dim 1: \", attention.sum(dim=1)[0])\n",
    "    print(\"Len Attention logits summ dim 2: \", len(attention.sum(dim=2)[0]))\n",
    "    print(\"Attention logits summ dim 2: \", attention.sum(dim=2)[0])\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in, dim_emb, dim_v):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_emb)\n",
    "        self.k = nn.Linear(dim_in, dim_emb)\n",
    "        self.v = nn.Linear(dim_in, dim_v)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        return scaled_dot_product(self.q(query), self.k(key), self.v(value))\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dim_in, dim_emb, dim_v):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_emb, dim_v) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_v, dim_in)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
    "        )\n",
    "    \n",
    "\n",
    "class MCABGRU(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 num_heads, \n",
    "                 dim_in, \n",
    "                 dim_emb,\n",
    "                 cb1_channels=(12,20), \n",
    "                 cb1_kernel_size=3, \n",
    "                 cb2_channels=(12,20),\n",
    "                 cb2_kernel_size=5,\n",
    "                 cb3_channels=(12,20), \n",
    "                 cb3_kernel_size=8,\n",
    "                 gru_hidden=16,\n",
    "                 dropout_p=0.4\n",
    "                ):\n",
    "        super(MCABGRU, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_k = dim_emb\n",
    "        self.cb1_channels = cb1_channels\n",
    "        self.cb1_kernel_size = cb1_kernel_size\n",
    "        self.cb2_channels = cb2_channels\n",
    "        self.cb2_kernel_size = cb2_kernel_size\n",
    "        self.cb3_channels = cb3_channels\n",
    "        self.cb3_kernel_size = cb3_kernel_size\n",
    "        self.gru_hidden=gru_hidden\n",
    "        self.p = dropout_p\n",
    "        \n",
    "        self.cb1 = self.convolution_block(self.cb1_channels, self.cb1_kernel_size)\n",
    "        self.cb2 = self.convolution_block(self.cb2_channels, self.cb2_kernel_size)\n",
    "        self.cb3 = self.convolution_block(self.cb3_channels, self.cb2_kernel_size)\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=self.num_heads, \n",
    "            dim_in=self.dim_in, \n",
    "            dim_emb=self.dim_emb, \n",
    "            dim_v=self.dim_emb)\n",
    "        self.gru = nn.GRU(input_size=2*self.cb3_channels[1], hidden_size=self.gru_hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.fc_gru = nn.Sequential(\n",
    "            nn.Linear(2*self.gru_hidden, 40), \n",
    "            nn.Tanh(), \n",
    "            nn.Dropout(self.p)\n",
    "        )\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(40, 16), \n",
    "            nn.Tanh(), \n",
    "            nn.Dropout(self.p), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def convolution_block(self, out_channels, kernel_size):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.in_channels, out_channels=out_channels[0], kernel_size=kernel_size, stride=1, padding='same'),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(in_channels=out_channels[0], out_channels=out_channels[1], kernel_size=kernel_size, stride=1, padding='same'),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x dims: 0: N(Batch Size), 1: L(Length of Signal), 2: N_Features or C_in(Input Channels)\n",
    "        print(\"Input shape: \", x.shape)\n",
    "        x_cnv_input = x.permute(0, 2, 1) #Conv1D expects: 0: N(Batch Size), 1: C_in(Input Channels), 2: L(Length of Signal)\n",
    "        print(\"Inpute shape transposed: \", x_cnv_input.shape)\n",
    "        cv1 = self.cb1(x_cnv_input)\n",
    "        cv2 = self.cb2(x_cnv_input)\n",
    "        print(\"Convolution output Block1: \", cv1.shape)\n",
    "        print(\"Convolution output Block2: \", cv2.shape)\n",
    "        cv1 = cv1.permute(0, 2, 1)\n",
    "        cv2 = cv2.permute(0, 2, 1)\n",
    "        print(\"Convolution output transposed Block1: \", cv1.shape)\n",
    "        print(\"Convolution output transposed Block2: \", cv2.shape)\n",
    "        x_cnv_sum = cv1+cv2 #dims: 0: N(Batch Size), 1: C_out(Input Channels), 2: L(Length of Signal)\n",
    "        print(\"Sum layer shape: \", x_cnv_sum.shape)\n",
    "        x_cnv3_output = self.cb3(x_cnv_input)\n",
    "        print(\"Convolution output Block3: \", x_cnv3_output.shape)\n",
    "        x_cnv3_output = x_cnv3_output.permute(0, 2, 1)\n",
    "        print(\"Convolution output transposed Block3: \", x_cnv3_output.shape)\n",
    "        x_attn_output = self.mha(x_cnv3_output, x_cnv3_output, x_cnv3_output)\n",
    "        print(\"Attention block output: \", x_attn_output.shape)\n",
    "        x_out = torch.cat((x_cnv_sum, x_attn_output), dim=2) #dims: 0: N(Batch Size), 1: 2*C_out(Input Channels), 2: L(Length of Signal)\n",
    "        #x_out = x_out.permute(0, 2, 1) #dims: 0: N(Batch Size), 1: L(Length of Signal) , 2: 2*C_out(Input Channels)\n",
    "        print(\"Concatenation of conv blocks: \", x_out.shape)\n",
    "        _, h_n = self.gru(x_out) #D∗num_layers, N, H_hidden\n",
    "        print(\"GRU last hidden state shape: \", h_n.shape)\n",
    "        h_n = h_n.permute(1, 0, 2)  #N, D∗num_layers, H_hidden\n",
    "        print(\"GRU permuted last hidden state shape: \", h_n.shape)\n",
    "        h_n = h_n.reshape(-1, 2*self.gru_hidden)\n",
    "        print(\"GRU reshaped last hidden state shape: \", h_n.shape)\n",
    "        x_out = self.fc_gru(h_n)\n",
    "        print(\"Output of FC from GRU shape: \", x_out.shape)\n",
    "        x_out = self.fc_out(x_out)\n",
    "        print(\"Output shape: \", x_out.shape)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9680e6a8-99b1-4a61-b05e-30ef871fc0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 30, 17])\n",
      "Input shape:  torch.Size([100, 30, 17])\n",
      "Inpute shape transposed:  torch.Size([100, 17, 30])\n",
      "Convolution output Block1:  torch.Size([100, 20, 30])\n",
      "Convolution output Block2:  torch.Size([100, 20, 30])\n",
      "Convolution output transposed Block1:  torch.Size([100, 30, 20])\n",
      "Convolution output transposed Block2:  torch.Size([100, 30, 20])\n",
      "Sum layer shape:  torch.Size([100, 30, 20])\n",
      "Convolution output Block3:  torch.Size([100, 20, 30])\n",
      "Convolution output transposed Block3:  torch.Size([100, 30, 20])\n",
      "V shape: torch.Size([100, 30, 16])\n",
      "Attention logits shape:  torch.Size([100, 30, 30])\n",
      "Len Attention logits summ dim 1:  30\n",
      "Attention logits summ dim 1:  tensor([1.0142, 1.0068, 1.0039, 1.0039, 1.0033, 1.0000, 0.9977, 0.9982, 1.0006,\n",
      "        0.9932, 0.9998, 1.0027, 0.9990, 0.9982, 1.0013, 1.0000, 0.9980, 1.0004,\n",
      "        0.9950, 1.0019, 0.9991, 1.0036, 0.9934, 0.9991, 1.0011, 0.9967, 0.9964,\n",
      "        0.9953, 0.9942, 1.0032], grad_fn=<SelectBackward0>)\n",
      "Len Attention logits summ dim 2:  30\n",
      "Attention logits summ dim 2:  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000], grad_fn=<SelectBackward0>)\n",
      "V shape: torch.Size([100, 30, 16])\n",
      "Attention logits shape:  torch.Size([100, 30, 30])\n",
      "Len Attention logits summ dim 1:  30\n",
      "Attention logits summ dim 1:  tensor([0.9988, 1.0002, 1.0021, 0.9992, 1.0022, 0.9985, 0.9978, 1.0016, 1.0015,\n",
      "        0.9981, 0.9973, 1.0021, 0.9984, 0.9997, 0.9997, 0.9979, 0.9986, 1.0000,\n",
      "        0.9982, 0.9984, 1.0020, 1.0020, 0.9977, 0.9995, 1.0001, 0.9996, 1.0017,\n",
      "        1.0053, 0.9959, 1.0057], grad_fn=<SelectBackward0>)\n",
      "Len Attention logits summ dim 2:  30\n",
      "Attention logits summ dim 2:  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000], grad_fn=<SelectBackward0>)\n",
      "Attention block output:  torch.Size([100, 30, 20])\n",
      "Concatenation of conv blocks:  torch.Size([100, 30, 40])\n",
      "GRU last hidden state shape:  torch.Size([2, 100, 16])\n",
      "GRU permuted last hidden state shape:  torch.Size([100, 2, 16])\n",
      "GRU reshaped last hidden state shape:  torch.Size([100, 32])\n",
      "Output of FC from GRU shape:  torch.Size([100, 40])\n",
      "Output shape:  torch.Size([100, 1])\n",
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "MAX_RUL=150\n",
    "\n",
    "trainset = CMAPSSSlidingWin(mode='train',\n",
    "                               data_path='./CMAPSSData/train_FD001.txt', max_rul=MAX_RUL, handcrafted=False)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CMAPSSSlidingWin(mode='test',\n",
    "                              data_path='./CMAPSSData/test_FD001.txt',\n",
    "                              rul_path='./CMAPSSData/RUL_FD001.txt',  max_rul=MAX_RUL, handcrafted=False)\n",
    "test_loader = DataLoader(dataset=testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "model = MCABGRU(\n",
    "     in_channels=17, \n",
    "     num_heads=2, \n",
    "     dim_in=20, \n",
    "     dim_emb=16, \n",
    "     cb1_channels=(12,20), \n",
    "     cb1_kernel_size=3, \n",
    "     cb2_channels=(12,20),\n",
    "     cb2_kernel_size=5,\n",
    "     cb3_channels=(12,20), \n",
    "     cb3_kernel_size=8,\n",
    "     gru_hidden=16,\n",
    "     dropout_p=0.4\n",
    "    )\n",
    "\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "print(x.shape)\n",
    "out = model.forward(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0ea328e4-9fcc-4667-bb3c-8cf3fdda864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch: 1 train loss: 39.822 val loss: 32.335 score: 12938.563\n",
      "Epoch: 2 train loss: 27.328 val loss: 30.460 score: 10007.907\n",
      "Epoch: 3 train loss: 24.372 val loss: 34.065 score: 15947.816\n",
      "Epoch: 4 train loss: 23.497 val loss: 26.020 score: 4416.489\n",
      "Epoch: 5 train loss: 22.797 val loss: 30.936 score: 10120.612\n",
      "Epoch: 6 train loss: 22.124 val loss: 32.041 score: 11900.935\n",
      "Epoch: 7 train loss: 21.977 val loss: 31.797 score: 11523.739\n",
      "Epoch: 8 train loss: 21.674 val loss: 35.643 score: 19598.660\n",
      "Epoch: 9 train loss: 21.496 val loss: 33.730 score: 15454.199\n",
      "Epoch: 10 train loss: 21.372 val loss: 35.481 score: 20472.734\n",
      "Epoch: 11 train loss: 21.336 val loss: 34.636 score: 17964.180\n",
      "Epoch: 12 train loss: 21.081 val loss: 33.716 score: 15819.551\n",
      "Epoch: 13 train loss: 21.077 val loss: 35.688 score: 21394.139\n",
      "Epoch: 14 train loss: 20.747 val loss: 31.512 score: 11377.028\n",
      "Epoch: 15 train loss: 20.686 val loss: 34.331 score: 16351.327\n",
      "Epoch: 16 train loss: 20.320 val loss: 34.096 score: 16606.688\n",
      "Epoch: 17 train loss: 20.513 val loss: 38.290 score: 27214.695\n",
      "Epoch: 18 train loss: 20.168 val loss: 31.091 score: 9565.270\n",
      "Epoch: 19 train loss: 20.155 val loss: 40.843 score: 40492.246\n",
      "Epoch: 20 train loss: 19.745 val loss: 34.528 score: 15368.188\n",
      "Epoch: 21 train loss: 19.767 val loss: 33.571 score: 15952.819\n",
      "Epoch: 22 train loss: 19.813 val loss: 32.321 score: 14420.112\n",
      "Epoch: 23 train loss: 19.509 val loss: 34.490 score: 21294.662\n",
      "Epoch: 24 train loss: 19.310 val loss: 36.468 score: 26455.262\n",
      "Epoch: 25 train loss: 19.324 val loss: 36.912 score: 28834.689\n",
      "Epoch: 26 train loss: 19.212 val loss: 37.877 score: 33807.266\n",
      "Epoch: 27 train loss: 19.113 val loss: 31.053 score: 13640.454\n",
      "Epoch: 28 train loss: 18.846 val loss: 34.962 score: 23254.863\n",
      "Epoch: 29 train loss: 18.923 val loss: 38.724 score: 38886.711\n",
      "Epoch: 30 train loss: 18.614 val loss: 34.316 score: 26250.947\n",
      "Epoch: 31 train loss: 18.528 val loss: 34.893 score: 26356.365\n",
      "Epoch: 32 train loss: 18.439 val loss: 34.727 score: 24050.326\n",
      "Epoch: 33 train loss: 18.177 val loss: 33.820 score: 20993.447\n",
      "Epoch: 34 train loss: 18.308 val loss: 30.786 score: 12114.689\n",
      "Epoch: 35 train loss: 18.007 val loss: 35.584 score: 31999.607\n",
      "Epoch: 36 train loss: 17.968 val loss: 37.884 score: 32930.598\n",
      "Epoch: 37 train loss: 17.746 val loss: 34.856 score: 21258.709\n",
      "Epoch: 38 train loss: 17.627 val loss: 29.382 score: 12266.010\n",
      "Epoch: 39 train loss: 17.631 val loss: 35.667 score: 25399.967\n",
      "Epoch: 40 train loss: 17.401 val loss: 35.126 score: 25856.518\n",
      "Epoch: 41 train loss: 17.402 val loss: 32.238 score: 18261.637\n",
      "Epoch: 42 train loss: 17.441 val loss: 35.545 score: 28833.264\n",
      "Epoch: 43 train loss: 17.303 val loss: 38.012 score: 39385.289\n",
      "Epoch: 44 train loss: 17.047 val loss: 35.942 score: 23959.107\n",
      "Epoch: 45 train loss: 16.991 val loss: 40.182 score: 46281.070\n",
      "Epoch: 46 train loss: 16.908 val loss: 34.235 score: 20675.527\n",
      "Epoch: 47 train loss: 16.821 val loss: 36.731 score: 35544.848\n",
      "Epoch: 48 train loss: 16.813 val loss: 38.950 score: 39639.086\n",
      "Epoch: 49 train loss: 16.609 val loss: 39.874 score: 43871.852\n",
      "Epoch: 50 train loss: 16.486 val loss: 34.039 score: 21545.918\n",
      "Epoch: 51 train loss: 16.418 val loss: 32.425 score: 18107.939\n",
      "Epoch: 52 train loss: 16.240 val loss: 40.233 score: 46417.918\n",
      "Epoch: 53 train loss: 16.143 val loss: 40.699 score: 60222.406\n",
      "Epoch: 54 train loss: 15.831 val loss: 38.782 score: 39299.008\n",
      "Epoch: 55 train loss: 15.958 val loss: 41.403 score: 62206.754\n",
      "Epoch: 56 train loss: 15.528 val loss: 37.826 score: 39718.691\n",
      "Epoch: 57 train loss: 15.490 val loss: 40.076 score: 57072.898\n",
      "Epoch: 58 train loss: 15.333 val loss: 37.945 score: 43215.996\n",
      "Epoch: 59 train loss: 15.359 val loss: 37.124 score: 39138.094\n",
      "Epoch: 60 train loss: 15.273 val loss: 32.603 score: 21673.555\n"
     ]
    }
   ],
   "source": [
    "MAX_RUL=150\n",
    "\n",
    "trainset = CMAPSSSlidingWin(mode='train',\n",
    "                               data_path='./CMAPSSData/train_FD001.txt', max_rul=MAX_RUL, handcrafted=False)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CMAPSSSlidingWin(mode='test',\n",
    "                              data_path='./CMAPSSData/test_FD001.txt',\n",
    "                              rul_path='./CMAPSSData/RUL_FD001.txt',  max_rul=MAX_RUL, handcrafted=False)\n",
    "test_loader = DataLoader(dataset=testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "model = MCABGRU(\n",
    "     in_channels=17, \n",
    "     num_heads=2, \n",
    "     dim_in=20, \n",
    "     dim_emb=16, \n",
    "     cb1_channels=(12,20), \n",
    "     cb1_kernel_size=3, \n",
    "     cb2_channels=(12,20),\n",
    "     cb2_kernel_size=5,\n",
    "     cb3_channels=(12,20), \n",
    "     cb3_kernel_size=8,\n",
    "     gru_hidden=16,\n",
    "     dropout_p=0.4\n",
    "    )\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 60\n",
    "\n",
    "trainer = TrainUtil(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=1)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd1b197-7977-49d4-972e-df181007dd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(((1,1),(1,1),(1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "056cd9b1-40a6-4859-9fad-8127c6339e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_2: \n",
      "\n",
      "torch.Size([5, 5]) \n",
      "\n",
      "tensor([[ 1.1367,  1.3423,  1.1382,  0.9341,  1.7746],\n",
      "        [-0.4897,  1.3324,  1.6550,  1.9777,  5.9321],\n",
      "        [-3.6484, -3.9453, -4.0308, -4.1162, -1.3557],\n",
      "        [ 1.9808,  0.0744, -0.2836, -0.6416, -2.8021],\n",
      "        [ 1.6505,  4.3517,  5.0366,  5.7215,  6.5347]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "input_2d = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]], dtype = torch.float)\n",
    "\n",
    "cnn1d_2 = nn.Conv1d(in_channels=2, out_channels=5, kernel_size=3, stride=1, padding='same')\n",
    "print(\"cnn1d_2: \\n\")\n",
    "print(cnn1d_2(input_2d).shape, \"\\n\")\n",
    "print(cnn1d_2(input_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d206a1-3297-4ca3-89c0-c4c19e2a8fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset load successfully!\n",
      "torch.Size([30, 17]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "MAX_RUL=150\n",
    "\n",
    "trainset = CMAPSSSlidingWin(mode='train',\n",
    "                               data_path='./CMAPSSData/train_FD001.txt', max_rul=MAX_RUL, standardize=True)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CMAPSSSlidingWin(mode='test',\n",
    "                              data_path='./CMAPSSData/test_FD001.txt',\n",
    "                              rul_path='./CMAPSSData/RUL_FD001.txt',  max_rul=MAX_RUL, standardize=True)\n",
    "test_loader = DataLoader(dataset=testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print('dataset load successfully!')\n",
    "print(next(iter(trainset))[0].shape, next(iter(trainset))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29818cd7-d996-4ca2-a40b-af8a6ab4610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_experiments(param_dict, n_runs=10, epochs=20, max_rul=150):\n",
    "    result ={'model_name': [], 'loss': [], 'score': []}\n",
    "    for model_name, item in tqdm(param_dict.items()):\n",
    "        handcrafted = item['handcrafted']\n",
    "        \n",
    "        for i in range(n_runs):\n",
    "            if model_name ==  'LSTM1':\n",
    "                model = LSTM1(input_size=17, seq_length=1, hidden_size=96, num_layers=4)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "            elif model_name == 'LSTMStackedHiddenStates':\n",
    "                model = LSTMStackedHiddenStates(input_size=17, time_steps=30, hidden_size=96, num_layers=4)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "            elif model_name == 'LSTMStackedHiddenStates2':\n",
    "                model = LSTMStackedHiddenStates2(input_size=17, time_steps=30, hidden_size=96, num_layers=4)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "            elif model_name == 'LSTMBaselineAtn':\n",
    "                model = LSTMBaselineAtn(n_features=17, n_hidden=96, time_steps=30)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "            elif model_name == 'LSTMHCAtn':\n",
    "                model = LSTMHCAtn()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "            else:\n",
    "                print(\"Model not implemented\")\n",
    "                continue\n",
    "            train_loader = item['train_loader']\n",
    "            test_loader = item['test_loader']\n",
    "            trainer = TrainUtil(model=model,\n",
    "                        optimizer=optimizer,\n",
    "                        train_loader=train_loader,\n",
    "                        test_loader=test_loader,\n",
    "                        max_rul=max_rul,\n",
    "                        verbosity=0, \n",
    "                        handcrafted = handcrafted)\n",
    "            history = trainer.train(epochs)\n",
    "            min_loss = min(history[\"val_loss\"])\n",
    "            min_score =  min(history[\"val_score\"]) \n",
    "\n",
    "            result['model_name'].append(model_name)\n",
    "            result['loss'].append(min_loss)\n",
    "            result['score'].append(min_score)\n",
    "    return  pd.DataFrame(data=result)          \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b77053d3-d07b-4702-a99a-bfaa57b3154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                  | 3/5 [37:57<25:37, 768.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: 100 30 96 hidden shape: 2 100 96\n",
      "\n",
      "output shape: 100 30 96 hidden shape: 2 100 96\n",
      "\n",
      "output shape: 100 30 96 hidden shape: 2 100 96\n",
      "\n",
      "output shape: 100 30 96 hidden shape: 2 100 96\n",
      "\n",
      "output shape: 100 30 96 hidden shape: 2 100 96\n",
      "\n",
      "output shape: 100 30 96 hidden shape: 2 100 96\n",
      "\n",
      "output shape: 100 30 96 hidden shape: 2 100 96\n",
      "\n",
      "output shape: 100 30 96 hidden shape: 2 100 96\n",
      "\n",
      "output shape: 100 30 96 hidden shape: 2 100 96\n",
      "\n",
      "output shape: 100 30 96 hidden shape: 2 100 96\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [58:51<00:00, 706.34s/it]\n"
     ]
    }
   ],
   "source": [
    "MAX_RUL=150\n",
    "epochs = 5\n",
    "n_runs = 3\n",
    "\n",
    "trainset = CMAPSSSlidingWin(mode='train',\n",
    "                               data_path='./CMAPSSData/train_FD001.txt', max_rul=MAX_RUL, standardize=True)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CMAPSSSlidingWin(mode='test',\n",
    "                              data_path='./CMAPSSData/test_FD001.txt',\n",
    "                              rul_path='./CMAPSSData/RUL_FD001.txt',  max_rul=MAX_RUL, standardize=True)\n",
    "test_loader = DataLoader(dataset=testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "trainset2 = CMAPSSSlidingWin(mode='train',\n",
    "                               data_path='./CMAPSSData/train_FD001.txt', max_rul=MAX_RUL, handcrafted=True)\n",
    "train_loader2 = DataLoader(dataset=trainset2, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset2 = CMAPSSSlidingWin(mode='test',\n",
    "                              data_path='./CMAPSSData/test_FD001.txt',\n",
    "                              rul_path='./CMAPSSData/RUL_FD001.txt',  max_rul=MAX_RUL, handcrafted=True)\n",
    "test_loader2 = DataLoader(dataset=testset2, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "param_dict = {'LSTM1':{'train_loader' : train_loader, \n",
    "                       'test_loader' : test_loader, \n",
    "                       'handcrafted': False},\n",
    "              'LSTMStackedHiddenStates':{'train_loader' : train_loader, \n",
    "                                         'test_loader' : test_loader, \n",
    "                                         'handcrafted': False},\n",
    "              'LSTMStackedHiddenStates2':{'train_loader' : train_loader, \n",
    "                                          'test_loader' : test_loader, \n",
    "                                          'handcrafted': False},\n",
    "              'LSTMBaselineAtn':{'train_loader' : train_loader, \n",
    "                                 'test_loader' : test_loader, \n",
    "                                 'handcrafted': False},\n",
    "              'LSTMHCAtn':{'train_loader' : train_loader2, \n",
    "                                 'test_loader' : test_loader2, \n",
    "                                 'handcrafted': True}\n",
    "             }\n",
    "\n",
    "\n",
    "df = run_experiments(param_dict, n_runs=10, epochs=80, max_rul=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6f9a1af-8cd1-4926-b809-96d1a612fb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>loss</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM1</td>\n",
       "      <td>26.535875</td>\n",
       "      <td>4373.493042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM1</td>\n",
       "      <td>29.963194</td>\n",
       "      <td>2933.383423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM1</td>\n",
       "      <td>29.460982</td>\n",
       "      <td>4432.286865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM1</td>\n",
       "      <td>30.429117</td>\n",
       "      <td>5871.550049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LSTM1</td>\n",
       "      <td>28.723947</td>\n",
       "      <td>3305.295532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LSTM1</td>\n",
       "      <td>28.401994</td>\n",
       "      <td>4074.789307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM1</td>\n",
       "      <td>27.618829</td>\n",
       "      <td>3260.043213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM1</td>\n",
       "      <td>26.210388</td>\n",
       "      <td>2782.662354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTM1</td>\n",
       "      <td>30.239519</td>\n",
       "      <td>8808.221191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LSTM1</td>\n",
       "      <td>41.651420</td>\n",
       "      <td>7373.315430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSTMStackedHiddenStates</td>\n",
       "      <td>25.881770</td>\n",
       "      <td>3436.112671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSTMStackedHiddenStates</td>\n",
       "      <td>24.983915</td>\n",
       "      <td>3105.133789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSTMStackedHiddenStates</td>\n",
       "      <td>26.148534</td>\n",
       "      <td>3483.148560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LSTMStackedHiddenStates</td>\n",
       "      <td>26.254597</td>\n",
       "      <td>3281.572998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LSTMStackedHiddenStates</td>\n",
       "      <td>25.468174</td>\n",
       "      <td>3686.002563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LSTMStackedHiddenStates</td>\n",
       "      <td>25.325798</td>\n",
       "      <td>2793.841431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LSTMStackedHiddenStates</td>\n",
       "      <td>28.187069</td>\n",
       "      <td>4966.958984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LSTMStackedHiddenStates</td>\n",
       "      <td>29.325218</td>\n",
       "      <td>6214.215332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LSTMStackedHiddenStates</td>\n",
       "      <td>25.483376</td>\n",
       "      <td>3264.176147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LSTMStackedHiddenStates</td>\n",
       "      <td>25.466411</td>\n",
       "      <td>3861.255127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LSTMStackedHiddenStates2</td>\n",
       "      <td>27.502262</td>\n",
       "      <td>5122.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LSTMStackedHiddenStates2</td>\n",
       "      <td>28.708100</td>\n",
       "      <td>6373.606201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LSTMStackedHiddenStates2</td>\n",
       "      <td>28.883027</td>\n",
       "      <td>7296.038330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LSTMStackedHiddenStates2</td>\n",
       "      <td>30.746752</td>\n",
       "      <td>8283.864502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LSTMStackedHiddenStates2</td>\n",
       "      <td>27.487100</td>\n",
       "      <td>4796.208496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LSTMStackedHiddenStates2</td>\n",
       "      <td>25.969825</td>\n",
       "      <td>3459.288452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LSTMStackedHiddenStates2</td>\n",
       "      <td>26.063264</td>\n",
       "      <td>4190.628418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LSTMStackedHiddenStates2</td>\n",
       "      <td>25.490038</td>\n",
       "      <td>3955.440552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LSTMStackedHiddenStates2</td>\n",
       "      <td>27.578154</td>\n",
       "      <td>5598.315430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LSTMStackedHiddenStates2</td>\n",
       "      <td>30.592449</td>\n",
       "      <td>10069.006348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LSTMBaselineAtn</td>\n",
       "      <td>33.310442</td>\n",
       "      <td>12327.917969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LSTMBaselineAtn</td>\n",
       "      <td>27.199497</td>\n",
       "      <td>5348.248169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LSTMBaselineAtn</td>\n",
       "      <td>28.603616</td>\n",
       "      <td>6615.650879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>LSTMBaselineAtn</td>\n",
       "      <td>32.228682</td>\n",
       "      <td>10389.837891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LSTMBaselineAtn</td>\n",
       "      <td>41.564079</td>\n",
       "      <td>7371.424316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>LSTMBaselineAtn</td>\n",
       "      <td>28.299813</td>\n",
       "      <td>5849.996826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LSTMBaselineAtn</td>\n",
       "      <td>30.978639</td>\n",
       "      <td>10023.427490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LSTMBaselineAtn</td>\n",
       "      <td>33.866495</td>\n",
       "      <td>14218.698730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LSTMBaselineAtn</td>\n",
       "      <td>25.723123</td>\n",
       "      <td>3790.465576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LSTMBaselineAtn</td>\n",
       "      <td>32.598610</td>\n",
       "      <td>9373.819336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>LSTMHCAtn</td>\n",
       "      <td>17.691170</td>\n",
       "      <td>590.025116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>LSTMHCAtn</td>\n",
       "      <td>17.103762</td>\n",
       "      <td>659.978271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>LSTMHCAtn</td>\n",
       "      <td>18.743555</td>\n",
       "      <td>834.048828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>LSTMHCAtn</td>\n",
       "      <td>17.151179</td>\n",
       "      <td>535.728729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>LSTMHCAtn</td>\n",
       "      <td>17.381524</td>\n",
       "      <td>587.332764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>LSTMHCAtn</td>\n",
       "      <td>17.889417</td>\n",
       "      <td>651.406952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>LSTMHCAtn</td>\n",
       "      <td>17.205837</td>\n",
       "      <td>583.402771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>LSTMHCAtn</td>\n",
       "      <td>17.454311</td>\n",
       "      <td>638.143890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>LSTMHCAtn</td>\n",
       "      <td>17.613602</td>\n",
       "      <td>623.382019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>LSTMHCAtn</td>\n",
       "      <td>17.643034</td>\n",
       "      <td>770.650085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model_name       loss         score\n",
       "0                      LSTM1  26.535875   4373.493042\n",
       "1                      LSTM1  29.963194   2933.383423\n",
       "2                      LSTM1  29.460982   4432.286865\n",
       "3                      LSTM1  30.429117   5871.550049\n",
       "4                      LSTM1  28.723947   3305.295532\n",
       "5                      LSTM1  28.401994   4074.789307\n",
       "6                      LSTM1  27.618829   3260.043213\n",
       "7                      LSTM1  26.210388   2782.662354\n",
       "8                      LSTM1  30.239519   8808.221191\n",
       "9                      LSTM1  41.651420   7373.315430\n",
       "10   LSTMStackedHiddenStates  25.881770   3436.112671\n",
       "11   LSTMStackedHiddenStates  24.983915   3105.133789\n",
       "12   LSTMStackedHiddenStates  26.148534   3483.148560\n",
       "13   LSTMStackedHiddenStates  26.254597   3281.572998\n",
       "14   LSTMStackedHiddenStates  25.468174   3686.002563\n",
       "15   LSTMStackedHiddenStates  25.325798   2793.841431\n",
       "16   LSTMStackedHiddenStates  28.187069   4966.958984\n",
       "17   LSTMStackedHiddenStates  29.325218   6214.215332\n",
       "18   LSTMStackedHiddenStates  25.483376   3264.176147\n",
       "19   LSTMStackedHiddenStates  25.466411   3861.255127\n",
       "20  LSTMStackedHiddenStates2  27.502262   5122.041016\n",
       "21  LSTMStackedHiddenStates2  28.708100   6373.606201\n",
       "22  LSTMStackedHiddenStates2  28.883027   7296.038330\n",
       "23  LSTMStackedHiddenStates2  30.746752   8283.864502\n",
       "24  LSTMStackedHiddenStates2  27.487100   4796.208496\n",
       "25  LSTMStackedHiddenStates2  25.969825   3459.288452\n",
       "26  LSTMStackedHiddenStates2  26.063264   4190.628418\n",
       "27  LSTMStackedHiddenStates2  25.490038   3955.440552\n",
       "28  LSTMStackedHiddenStates2  27.578154   5598.315430\n",
       "29  LSTMStackedHiddenStates2  30.592449  10069.006348\n",
       "30           LSTMBaselineAtn  33.310442  12327.917969\n",
       "31           LSTMBaselineAtn  27.199497   5348.248169\n",
       "32           LSTMBaselineAtn  28.603616   6615.650879\n",
       "33           LSTMBaselineAtn  32.228682  10389.837891\n",
       "34           LSTMBaselineAtn  41.564079   7371.424316\n",
       "35           LSTMBaselineAtn  28.299813   5849.996826\n",
       "36           LSTMBaselineAtn  30.978639  10023.427490\n",
       "37           LSTMBaselineAtn  33.866495  14218.698730\n",
       "38           LSTMBaselineAtn  25.723123   3790.465576\n",
       "39           LSTMBaselineAtn  32.598610   9373.819336\n",
       "40                 LSTMHCAtn  17.691170    590.025116\n",
       "41                 LSTMHCAtn  17.103762    659.978271\n",
       "42                 LSTMHCAtn  18.743555    834.048828\n",
       "43                 LSTMHCAtn  17.151179    535.728729\n",
       "44                 LSTMHCAtn  17.381524    587.332764\n",
       "45                 LSTMHCAtn  17.889417    651.406952\n",
       "46                 LSTMHCAtn  17.205837    583.402771\n",
       "47                 LSTMHCAtn  17.454311    638.143890\n",
       "48                 LSTMHCAtn  17.613602    623.382019\n",
       "49                 LSTMHCAtn  17.643034    770.650085"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831241f-614b-4361-9404-e8c76d48511c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d61e577-719b-4465-a7d5-d478735f5e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [1,2,3,4]\n",
    "w[-1:]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d05c1f-04a8-4b64-937b-7a0897a5d7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0687882f-fe25-47e0-a332-44c835683086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch: 1 train loss: 36.994 val loss: 30.615 score: 12186.923\n",
      "Epoch: 2 train loss: 27.372 val loss: 28.522 score: 8214.798\n",
      "Epoch: 3 train loss: 26.452 val loss: 31.503 score: 12044.242\n",
      "Epoch: 4 train loss: 25.924 val loss: 32.308 score: 13199.387\n",
      "Epoch: 5 train loss: 25.736 val loss: 30.993 score: 10266.152\n",
      "Epoch: 6 train loss: 25.438 val loss: 32.832 score: 13162.540\n",
      "Epoch: 7 train loss: 25.429 val loss: 30.069 score: 8423.285\n",
      "Epoch: 8 train loss: 25.341 val loss: 32.769 score: 12484.694\n",
      "Epoch: 9 train loss: 25.366 val loss: 32.914 score: 13344.917\n",
      "Epoch: 10 train loss: 25.321 val loss: 33.654 score: 14335.725\n",
      "Epoch: 11 train loss: 25.191 val loss: 33.812 score: 15577.949\n",
      "Epoch: 12 train loss: 25.249 val loss: 33.583 score: 14304.948\n",
      "Epoch: 13 train loss: 25.194 val loss: 31.589 score: 11023.808\n",
      "Epoch: 14 train loss: 25.087 val loss: 33.238 score: 14728.107\n",
      "Epoch: 15 train loss: 25.259 val loss: 35.317 score: 19128.981\n",
      "Epoch: 16 train loss: 25.216 val loss: 31.834 score: 11715.312\n",
      "Epoch: 17 train loss: 25.178 val loss: 34.691 score: 17720.364\n",
      "Epoch: 18 train loss: 25.122 val loss: 30.544 score: 9872.234\n",
      "Epoch: 19 train loss: 25.146 val loss: 31.902 score: 12333.450\n",
      "Epoch: 20 train loss: 25.188 val loss: 32.332 score: 13052.534\n",
      "Epoch: 21 train loss: 25.254 val loss: 30.923 score: 10235.495\n",
      "Epoch: 22 train loss: 25.238 val loss: 32.764 score: 13968.736\n",
      "Epoch: 23 train loss: 25.275 val loss: 32.918 score: 13901.807\n",
      "Epoch: 24 train loss: 25.094 val loss: 34.033 score: 16620.644\n",
      "Epoch: 25 train loss: 25.083 val loss: 33.334 score: 15264.281\n",
      "Epoch: 26 train loss: 25.200 val loss: 35.806 score: 21473.230\n",
      "Epoch: 27 train loss: 25.170 val loss: 33.561 score: 15777.122\n",
      "Epoch: 28 train loss: 25.141 val loss: 34.250 score: 17426.475\n",
      "Epoch: 29 train loss: 25.176 val loss: 34.423 score: 17818.346\n",
      "Epoch: 30 train loss: 25.048 val loss: 32.907 score: 14569.059\n"
     ]
    }
   ],
   "source": [
    "model = LSTM1(input_size=17, seq_length=1, hidden_size=96, num_layers=4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 30\n",
    "\n",
    "trainer = TrainUtil(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78941a3e-9a0b-4628-9bae-edfe04dba48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch: 1 train loss: 37.658 val loss: 33.937 score: 20437.864\n",
      "Epoch: 2 train loss: 27.302 val loss: 32.196 score: 17786.137\n",
      "Epoch: 3 train loss: 24.310 val loss: 32.647 score: 19388.890\n",
      "Epoch: 4 train loss: 23.229 val loss: 31.414 score: 12352.909\n",
      "Epoch: 5 train loss: 22.567 val loss: 32.714 score: 14543.589\n",
      "Epoch: 6 train loss: 21.807 val loss: 29.101 score: 7894.909\n",
      "Epoch: 7 train loss: 20.963 val loss: 33.775 score: 14894.885\n",
      "Epoch: 8 train loss: 20.509 val loss: 30.342 score: 9874.956\n",
      "Epoch: 9 train loss: 19.609 val loss: 32.539 score: 10718.567\n",
      "Epoch: 10 train loss: 19.139 val loss: 32.140 score: 10829.394\n",
      "Epoch: 11 train loss: 18.524 val loss: 33.307 score: 13092.397\n",
      "Epoch: 12 train loss: 18.204 val loss: 32.502 score: 10608.090\n",
      "Epoch: 13 train loss: 17.267 val loss: 31.955 score: 9720.194\n",
      "Epoch: 14 train loss: 17.201 val loss: 30.874 score: 7813.583\n",
      "Epoch: 15 train loss: 16.456 val loss: 31.037 score: 10163.430\n",
      "Epoch: 16 train loss: 16.307 val loss: 32.661 score: 10688.232\n",
      "Epoch: 17 train loss: 16.108 val loss: 31.540 score: 9732.427\n",
      "Epoch: 18 train loss: 15.786 val loss: 30.774 score: 8944.965\n",
      "Epoch: 19 train loss: 15.537 val loss: 28.869 score: 5957.601\n",
      "Epoch: 20 train loss: 15.317 val loss: 30.631 score: 8692.100\n",
      "Epoch: 21 train loss: 15.267 val loss: 31.239 score: 9290.219\n",
      "Epoch: 22 train loss: 15.273 val loss: 30.661 score: 7906.075\n",
      "Epoch: 23 train loss: 15.512 val loss: 31.569 score: 8937.410\n",
      "Epoch: 24 train loss: 14.904 val loss: 30.288 score: 7411.417\n",
      "Epoch: 25 train loss: 14.730 val loss: 29.869 score: 7178.750\n",
      "Epoch: 26 train loss: 14.455 val loss: 28.990 score: 7537.138\n",
      "Epoch: 27 train loss: 14.622 val loss: 30.045 score: 7239.221\n",
      "Epoch: 28 train loss: 14.480 val loss: 30.396 score: 7804.580\n",
      "Epoch: 29 train loss: 14.548 val loss: 32.150 score: 11531.090\n",
      "Epoch: 30 train loss: 14.307 val loss: 30.147 score: 6783.422\n",
      "Epoch: 31 train loss: 14.429 val loss: 30.650 score: 8301.890\n",
      "Epoch: 32 train loss: 14.255 val loss: 30.166 score: 8334.939\n",
      "Epoch: 33 train loss: 13.847 val loss: 30.943 score: 9603.133\n",
      "Epoch: 34 train loss: 13.884 val loss: 30.705 score: 9718.767\n",
      "Epoch: 35 train loss: 13.962 val loss: 30.114 score: 7456.497\n",
      "Epoch: 36 train loss: 13.881 val loss: 30.664 score: 7352.596\n",
      "Epoch: 37 train loss: 14.031 val loss: 30.439 score: 9863.970\n",
      "Epoch: 38 train loss: 13.957 val loss: 29.725 score: 6040.728\n",
      "Epoch: 39 train loss: 13.890 val loss: 31.251 score: 8197.928\n",
      "Epoch: 40 train loss: 13.688 val loss: 29.845 score: 7711.060\n",
      "Epoch: 41 train loss: 13.902 val loss: 31.358 score: 7809.114\n",
      "Epoch: 42 train loss: 13.699 val loss: 31.668 score: 9707.835\n",
      "Epoch: 43 train loss: 13.556 val loss: 29.330 score: 6350.552\n",
      "Epoch: 44 train loss: 13.902 val loss: 32.183 score: 9224.652\n",
      "Epoch: 45 train loss: 15.709 val loss: 30.794 score: 6381.845\n",
      "Epoch: 46 train loss: 13.844 val loss: 30.041 score: 5979.746\n",
      "Epoch: 47 train loss: 13.550 val loss: 30.306 score: 7018.786\n",
      "Epoch: 48 train loss: 13.690 val loss: 29.993 score: 6009.436\n",
      "Epoch: 49 train loss: 13.825 val loss: 29.186 score: 5710.303\n",
      "Epoch: 50 train loss: 13.563 val loss: 31.911 score: 9613.510\n",
      "Epoch: 51 train loss: 13.425 val loss: 31.535 score: 8840.109\n",
      "Epoch: 52 train loss: 13.480 val loss: 29.830 score: 6441.586\n",
      "Epoch: 53 train loss: 13.424 val loss: 31.334 score: 8884.156\n",
      "Epoch: 54 train loss: 13.436 val loss: 28.260 score: 5012.584\n",
      "Epoch: 55 train loss: 13.643 val loss: 31.139 score: 7588.669\n",
      "Epoch: 56 train loss: 13.526 val loss: 32.378 score: 10474.314\n",
      "Epoch: 57 train loss: 13.516 val loss: 31.272 score: 8208.741\n",
      "Epoch: 58 train loss: 13.386 val loss: 30.242 score: 6182.394\n",
      "Epoch: 59 train loss: 13.601 val loss: 31.741 score: 8586.916\n",
      "Epoch: 60 train loss: 13.427 val loss: 30.447 score: 6628.971\n"
     ]
    }
   ],
   "source": [
    "model = LSTMStackedHiddenStates(input_size=17, time_steps=30, hidden_size=96, num_layers=4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 60\n",
    "\n",
    "trainer = TrainUtil(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23cfb724-b2ab-44b2-b415-c60a58e53fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch: 1 train loss: 91.787 val loss: 77.238 score: 4067666.176\n",
      "Epoch: 2 train loss: 89.442 val loss: 75.061 score: 3136825.551\n",
      "Epoch: 3 train loss: 87.286 val loss: 72.944 score: 2428801.932\n",
      "Epoch: 4 train loss: 85.178 val loss: 70.993 score: 1887480.373\n",
      "Epoch: 5 train loss: 83.075 val loss: 69.296 score: 1469734.026\n",
      "Epoch: 6 train loss: 48.748 val loss: 37.104 score: 30467.937\n",
      "Epoch: 7 train loss: 36.722 val loss: 36.970 score: 25122.133\n",
      "Epoch: 8 train loss: 34.453 val loss: 34.543 score: 20362.019\n",
      "Epoch: 9 train loss: 32.883 val loss: 35.140 score: 20869.518\n",
      "Epoch: 10 train loss: 31.954 val loss: 33.066 score: 15641.128\n",
      "Epoch: 11 train loss: 31.370 val loss: 33.495 score: 16595.260\n",
      "Epoch: 12 train loss: 30.758 val loss: 33.671 score: 15227.735\n",
      "Epoch: 13 train loss: 30.766 val loss: 33.208 score: 16680.792\n",
      "Epoch: 14 train loss: 30.081 val loss: 34.630 score: 16340.863\n",
      "Epoch: 15 train loss: 30.228 val loss: 33.699 score: 15951.567\n",
      "Epoch: 16 train loss: 29.068 val loss: 33.486 score: 17395.401\n",
      "Epoch: 17 train loss: 29.006 val loss: 34.293 score: 17091.587\n",
      "Epoch: 18 train loss: 28.650 val loss: 33.192 score: 17326.606\n",
      "Epoch: 19 train loss: 28.368 val loss: 33.489 score: 13593.752\n",
      "Epoch: 20 train loss: 28.229 val loss: 34.335 score: 15488.823\n",
      "Epoch: 21 train loss: 27.684 val loss: 32.333 score: 13333.984\n",
      "Epoch: 22 train loss: 27.502 val loss: 32.970 score: 13982.775\n",
      "Epoch: 23 train loss: 27.081 val loss: 33.537 score: 14998.302\n",
      "Epoch: 24 train loss: 27.349 val loss: 32.004 score: 12343.164\n",
      "Epoch: 25 train loss: 26.627 val loss: 33.350 score: 13142.607\n",
      "Epoch: 26 train loss: 26.321 val loss: 33.209 score: 13456.521\n",
      "Epoch: 27 train loss: 26.070 val loss: 31.954 score: 10997.474\n",
      "Epoch: 28 train loss: 25.739 val loss: 31.198 score: 10631.933\n",
      "Epoch: 29 train loss: 25.524 val loss: 30.577 score: 9851.611\n",
      "Epoch: 30 train loss: 25.198 val loss: 32.538 score: 11467.653\n",
      "Epoch: 31 train loss: 25.010 val loss: 31.125 score: 10216.054\n",
      "Epoch: 32 train loss: 24.172 val loss: 31.544 score: 9678.574\n",
      "Epoch: 33 train loss: 24.421 val loss: 30.485 score: 9108.013\n",
      "Epoch: 34 train loss: 24.071 val loss: 30.758 score: 8691.034\n",
      "Epoch: 35 train loss: 23.608 val loss: 30.551 score: 8870.330\n",
      "Epoch: 36 train loss: 23.483 val loss: 30.396 score: 8954.952\n",
      "Epoch: 37 train loss: 23.198 val loss: 31.404 score: 9742.016\n",
      "Epoch: 38 train loss: 23.028 val loss: 29.680 score: 7931.576\n",
      "Epoch: 39 train loss: 22.790 val loss: 30.574 score: 8919.983\n",
      "Epoch: 40 train loss: 22.487 val loss: 29.954 score: 7680.387\n",
      "Epoch: 41 train loss: 21.952 val loss: 28.906 score: 6421.374\n",
      "Epoch: 42 train loss: 21.824 val loss: 30.146 score: 7876.118\n",
      "Epoch: 43 train loss: 21.852 val loss: 29.477 score: 7298.750\n",
      "Epoch: 44 train loss: 21.423 val loss: 30.118 score: 7738.354\n",
      "Epoch: 45 train loss: 21.027 val loss: 27.815 score: 5184.058\n",
      "Epoch: 46 train loss: 21.100 val loss: 28.147 score: 5845.032\n",
      "Epoch: 47 train loss: 20.795 val loss: 28.937 score: 6656.853\n",
      "Epoch: 48 train loss: 20.558 val loss: 27.541 score: 5155.953\n",
      "Epoch: 49 train loss: 20.275 val loss: 27.904 score: 5343.519\n",
      "Epoch: 50 train loss: 20.077 val loss: 27.647 score: 5184.767\n",
      "Epoch: 51 train loss: 20.274 val loss: 27.845 score: 5333.676\n",
      "Epoch: 52 train loss: 20.139 val loss: 27.013 score: 4765.989\n",
      "Epoch: 53 train loss: 19.861 val loss: 27.414 score: 4918.588\n",
      "Epoch: 54 train loss: 20.004 val loss: 27.304 score: 4821.664\n",
      "Epoch: 55 train loss: 19.382 val loss: 27.378 score: 4630.762\n",
      "Epoch: 56 train loss: 19.740 val loss: 26.654 score: 4357.864\n",
      "Epoch: 57 train loss: 19.298 val loss: 26.607 score: 4209.453\n",
      "Epoch: 58 train loss: 19.105 val loss: 27.540 score: 5184.299\n",
      "Epoch: 59 train loss: 19.209 val loss: 27.944 score: 5168.494\n",
      "Epoch: 60 train loss: 18.977 val loss: 27.690 score: 5019.374\n"
     ]
    }
   ],
   "source": [
    "model = LSTMStackedHiddenStates2(input_size=17, time_steps=30, hidden_size=96, num_layers=4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 60\n",
    "\n",
    "trainer = TrainUtil(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0688dc72-7ee2-4ad5-8320-3399445b2853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "output shape: [100, 30, 32] hidden shape: [4, 100, 32]\n",
      "Epoch: 1 train loss: 72.064 val loss: 50.359 score: 187879.227\n",
      "Epoch: 2 train loss: 51.866 val loss: 41.668 score: 78857.988\n",
      "Epoch: 3 train loss: 37.421 val loss: 39.925 score: 48641.953\n",
      "Epoch: 4 train loss: 30.035 val loss: 42.444 score: 91043.059\n",
      "Epoch: 5 train loss: 27.446 val loss: 43.399 score: 98215.130\n",
      "Epoch: 6 train loss: 26.735 val loss: 44.140 score: 75467.475\n",
      "Epoch: 7 train loss: 26.246 val loss: 45.169 score: 79697.635\n",
      "Epoch: 8 train loss: 25.830 val loss: 43.533 score: 59022.138\n",
      "Epoch: 9 train loss: 25.720 val loss: 42.915 score: 48805.940\n",
      "Epoch: 10 train loss: 25.286 val loss: 45.185 score: 59679.663\n",
      "Epoch: 11 train loss: 25.171 val loss: 42.806 score: 48484.895\n",
      "Epoch: 12 train loss: 24.387 val loss: 45.775 score: 73346.375\n",
      "Epoch: 13 train loss: 23.988 val loss: 48.468 score: 101880.016\n",
      "Epoch: 14 train loss: 24.286 val loss: 48.089 score: 81174.062\n",
      "Epoch: 15 train loss: 23.754 val loss: 45.530 score: 55214.076\n",
      "Epoch: 16 train loss: 23.784 val loss: 44.885 score: 48235.466\n",
      "Epoch: 17 train loss: 23.641 val loss: 44.517 score: 45618.268\n",
      "Epoch: 18 train loss: 23.413 val loss: 44.263 score: 44865.727\n",
      "Epoch: 19 train loss: 22.995 val loss: 44.735 score: 45989.421\n",
      "Epoch: 20 train loss: 22.979 val loss: 45.588 score: 53737.938\n",
      "Epoch: 21 train loss: 22.975 val loss: 45.402 score: 51196.675\n",
      "Epoch: 22 train loss: 22.843 val loss: 44.505 score: 48283.462\n",
      "Epoch: 23 train loss: 22.931 val loss: 43.704 score: 39656.650\n",
      "Epoch: 24 train loss: 22.857 val loss: 46.096 score: 53454.666\n",
      "Epoch: 25 train loss: 22.844 val loss: 43.646 score: 38174.215\n",
      "Epoch: 26 train loss: 22.637 val loss: 43.761 score: 39197.270\n",
      "Epoch: 27 train loss: 22.819 val loss: 45.010 score: 46077.094\n",
      "Epoch: 28 train loss: 22.352 val loss: 46.314 score: 53113.083\n",
      "Epoch: 29 train loss: 22.337 val loss: 46.372 score: 59577.231\n",
      "Epoch: 30 train loss: 22.472 val loss: 45.584 score: 49056.524\n",
      "Epoch: 31 train loss: 22.416 val loss: 43.069 score: 35037.392\n",
      "Epoch: 32 train loss: 22.251 val loss: 45.879 score: 49684.660\n",
      "Epoch: 33 train loss: 22.382 val loss: 45.436 score: 47457.422\n",
      "Epoch: 34 train loss: 22.091 val loss: 46.719 score: 54944.246\n",
      "Epoch: 35 train loss: 22.436 val loss: 46.340 score: 49775.151\n",
      "Epoch: 36 train loss: 22.378 val loss: 45.882 score: 49024.259\n",
      "Epoch: 37 train loss: 22.315 val loss: 44.686 score: 39812.882\n",
      "Epoch: 38 train loss: 21.745 val loss: 44.538 score: 38643.893\n",
      "Epoch: 39 train loss: 22.494 val loss: 44.512 score: 37782.983\n",
      "Epoch: 40 train loss: 22.410 val loss: 44.306 score: 38176.799\n",
      "Epoch: 41 train loss: 21.938 val loss: 44.738 score: 41376.479\n",
      "Epoch: 42 train loss: 21.995 val loss: 45.115 score: 42414.320\n",
      "Epoch: 43 train loss: 21.980 val loss: 45.540 score: 47157.493\n",
      "Epoch: 44 train loss: 22.280 val loss: 44.763 score: 41714.119\n",
      "Epoch: 45 train loss: 22.254 val loss: 46.038 score: 49531.586\n",
      "Epoch: 46 train loss: 21.912 val loss: 44.480 score: 39385.598\n",
      "Epoch: 47 train loss: 22.079 val loss: 45.049 score: 43490.838\n",
      "Epoch: 48 train loss: 21.961 val loss: 43.909 score: 35835.384\n",
      "Epoch: 49 train loss: 21.978 val loss: 44.773 score: 43300.644\n",
      "Epoch: 50 train loss: 22.269 val loss: 43.528 score: 34496.608\n",
      "Epoch: 51 train loss: 22.224 val loss: 44.439 score: 40367.501\n",
      "Epoch: 52 train loss: 22.223 val loss: 42.916 score: 33986.471\n",
      "Epoch: 53 train loss: 21.969 val loss: 45.291 score: 45575.572\n",
      "Epoch: 54 train loss: 22.432 val loss: 45.496 score: 47627.210\n",
      "Epoch: 55 train loss: 22.289 val loss: 45.440 score: 48669.579\n",
      "Epoch: 56 train loss: 21.933 val loss: 44.648 score: 40701.345\n",
      "Epoch: 57 train loss: 21.959 val loss: 44.691 score: 40153.649\n",
      "Epoch: 58 train loss: 21.998 val loss: 45.394 score: 50639.689\n",
      "Epoch: 59 train loss: 22.016 val loss: 44.951 score: 42800.489\n",
      "Epoch: 60 train loss: 22.094 val loss: 45.307 score: 44531.402\n"
     ]
    }
   ],
   "source": [
    "model = LSTMBaseline(n_features=17, n_hidden=32, n_layers=4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 60\n",
    "\n",
    "trainer = TrainUtil(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e667e3e0-2d0f-4163-9170-b5d2b72f1c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "output shape: 100 30 96 hidden shape: 2 100 96\n",
      "\n",
      "Epoch: 1 train loss: 61.900 val loss: 45.053 score: 45419.562\n",
      "Epoch: 2 train loss: 52.418 val loss: 39.077 score: 20406.477\n",
      "Epoch: 3 train loss: 40.591 val loss: 36.914 score: 39984.907\n",
      "Epoch: 4 train loss: 31.544 val loss: 35.144 score: 35339.014\n",
      "Epoch: 5 train loss: 27.596 val loss: 33.484 score: 23988.798\n",
      "Epoch: 6 train loss: 24.993 val loss: 31.644 score: 17231.094\n",
      "Epoch: 7 train loss: 23.520 val loss: 32.383 score: 16953.878\n",
      "Epoch: 8 train loss: 22.482 val loss: 31.475 score: 14895.263\n",
      "Epoch: 9 train loss: 21.877 val loss: 31.022 score: 13837.921\n",
      "Epoch: 10 train loss: 21.625 val loss: 30.087 score: 10711.819\n",
      "Epoch: 11 train loss: 20.763 val loss: 30.511 score: 11083.618\n",
      "Epoch: 12 train loss: 20.628 val loss: 29.275 score: 9364.439\n",
      "Epoch: 13 train loss: 20.417 val loss: 29.266 score: 9123.147\n",
      "Epoch: 14 train loss: 19.763 val loss: 29.094 score: 8455.182\n",
      "Epoch: 15 train loss: 19.587 val loss: 29.114 score: 8180.497\n",
      "Epoch: 16 train loss: 19.251 val loss: 28.726 score: 7678.963\n",
      "Epoch: 17 train loss: 18.733 val loss: 29.556 score: 8029.111\n",
      "Epoch: 18 train loss: 18.332 val loss: 28.304 score: 6984.497\n",
      "Epoch: 19 train loss: 18.243 val loss: 28.673 score: 6934.878\n",
      "Epoch: 20 train loss: 17.677 val loss: 28.185 score: 6649.346\n",
      "Epoch: 21 train loss: 17.491 val loss: 28.128 score: 6099.268\n",
      "Epoch: 22 train loss: 17.231 val loss: 28.488 score: 6383.021\n",
      "Epoch: 23 train loss: 17.030 val loss: 28.466 score: 5966.392\n",
      "Epoch: 24 train loss: 16.609 val loss: 27.736 score: 5635.123\n",
      "Epoch: 25 train loss: 16.517 val loss: 28.432 score: 5821.043\n",
      "Epoch: 26 train loss: 15.922 val loss: 26.877 score: 4870.903\n",
      "Epoch: 27 train loss: 15.882 val loss: 27.657 score: 5418.835\n",
      "Epoch: 28 train loss: 15.459 val loss: 27.793 score: 5293.911\n",
      "Epoch: 29 train loss: 15.440 val loss: 26.905 score: 4850.265\n",
      "Epoch: 30 train loss: 14.979 val loss: 27.162 score: 5063.264\n",
      "Epoch: 31 train loss: 14.923 val loss: 28.202 score: 5711.335\n",
      "Epoch: 32 train loss: 14.769 val loss: 27.453 score: 5203.015\n",
      "Epoch: 33 train loss: 14.547 val loss: 26.145 score: 4235.689\n",
      "Epoch: 34 train loss: 14.148 val loss: 27.873 score: 5687.736\n",
      "Epoch: 35 train loss: 14.067 val loss: 26.797 score: 4796.244\n",
      "Epoch: 36 train loss: 13.730 val loss: 27.569 score: 5163.002\n",
      "Epoch: 37 train loss: 13.471 val loss: 27.094 score: 4971.254\n",
      "Epoch: 38 train loss: 13.088 val loss: 26.916 score: 5262.400\n",
      "Epoch: 39 train loss: 13.173 val loss: 27.389 score: 5503.309\n",
      "Epoch: 40 train loss: 12.933 val loss: 26.605 score: 4928.350\n",
      "Epoch: 41 train loss: 12.408 val loss: 26.907 score: 4981.084\n",
      "Epoch: 42 train loss: 12.288 val loss: 26.469 score: 4617.489\n",
      "Epoch: 43 train loss: 12.366 val loss: 26.933 score: 4918.770\n",
      "Epoch: 44 train loss: 11.828 val loss: 26.693 score: 4956.891\n",
      "Epoch: 45 train loss: 11.778 val loss: 26.721 score: 4892.551\n",
      "Epoch: 46 train loss: 11.703 val loss: 26.015 score: 4627.735\n",
      "Epoch: 47 train loss: 11.563 val loss: 26.295 score: 4744.381\n",
      "Epoch: 48 train loss: 11.334 val loss: 26.192 score: 4604.386\n",
      "Epoch: 49 train loss: 11.186 val loss: 26.188 score: 4584.823\n",
      "Epoch: 50 train loss: 10.893 val loss: 26.262 score: 4603.763\n",
      "Epoch: 51 train loss: 10.810 val loss: 26.346 score: 4660.942\n",
      "Epoch: 52 train loss: 10.408 val loss: 25.887 score: 4208.781\n",
      "Epoch: 53 train loss: 10.366 val loss: 25.662 score: 4133.004\n",
      "Epoch: 54 train loss: 10.539 val loss: 26.067 score: 4279.904\n",
      "Epoch: 55 train loss: 10.215 val loss: 25.580 score: 4220.801\n",
      "Epoch: 56 train loss: 9.929 val loss: 26.039 score: 4285.919\n",
      "Epoch: 57 train loss: 9.690 val loss: 25.929 score: 4278.021\n",
      "Epoch: 58 train loss: 9.789 val loss: 25.850 score: 4077.212\n",
      "Epoch: 59 train loss: 9.659 val loss: 25.485 score: 3777.115\n",
      "Epoch: 60 train loss: 9.501 val loss: 25.618 score: 4016.432\n"
     ]
    }
   ],
   "source": [
    "model = LSTMBaselineAtn(n_features=17, n_hidden=96, time_steps=30)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 60\n",
    "\n",
    "trainer = TrainUtil(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0455ce7-c1c6-4179-8f1e-e0b98d88c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset load successfully!\n",
      "torch.Size([30, 17]) torch.Size([34])\n"
     ]
    }
   ],
   "source": [
    "MAX_RUL=150\n",
    "\n",
    "trainset = CMAPSSSlidingWin(mode='train',\n",
    "                               data_path='./CMAPSSData/train_FD001.txt', max_rul=MAX_RUL, handcrafted=True)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CMAPSSSlidingWin(mode='test',\n",
    "                              data_path='./CMAPSSData/test_FD001.txt',\n",
    "                              rul_path='./CMAPSSData/RUL_FD001.txt',  max_rul=MAX_RUL, handcrafted=True)\n",
    "test_loader = DataLoader(dataset=testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print('dataset load successfully!')\n",
    "print(next(iter(trainset))[0].shape, next(iter(trainset))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2976766-945b-45c1-8578-bfccccf25b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17731,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "782afaaf-59a2-40cb-b528-ba3d0a0c9076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 30, 17)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "353fc243-ec3c-4e0e-8565-c57dfa959509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 34)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.hc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4aab027-0720-4154-be71-31924cd33b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch: 1 train loss: 124.394 val loss: 58.608 score: 389865.750\n",
      "Epoch: 2 train loss: 51.384 val loss: 31.852 score: 2190.425\n",
      "Epoch: 3 train loss: 39.413 val loss: 24.693 score: 994.842\n",
      "Epoch: 4 train loss: 35.094 val loss: 22.373 score: 792.304\n",
      "Epoch: 5 train loss: 33.798 val loss: 22.723 score: 787.488\n",
      "Epoch: 6 train loss: 32.420 val loss: 22.046 score: 737.875\n",
      "Epoch: 7 train loss: 31.509 val loss: 21.896 score: 709.553\n",
      "Epoch: 8 train loss: 30.397 val loss: 20.895 score: 651.857\n",
      "Epoch: 9 train loss: 29.817 val loss: 20.732 score: 622.582\n",
      "Epoch: 10 train loss: 28.607 val loss: 19.744 score: 585.714\n",
      "Epoch: 11 train loss: 28.027 val loss: 19.680 score: 564.755\n",
      "Epoch: 12 train loss: 27.526 val loss: 19.178 score: 552.295\n",
      "Epoch: 13 train loss: 26.853 val loss: 18.968 score: 542.401\n",
      "Epoch: 14 train loss: 26.719 val loss: 18.854 score: 550.246\n",
      "Epoch: 15 train loss: 26.407 val loss: 18.651 score: 545.009\n",
      "Epoch: 16 train loss: 26.248 val loss: 18.719 score: 551.912\n",
      "Epoch: 17 train loss: 25.883 val loss: 17.554 score: 555.947\n",
      "Epoch: 18 train loss: 25.627 val loss: 17.179 score: 573.860\n",
      "Epoch: 19 train loss: 25.219 val loss: 16.789 score: 598.544\n",
      "Epoch: 20 train loss: 24.661 val loss: 16.794 score: 606.185\n"
     ]
    }
   ],
   "source": [
    "model = LSTMHCAtn()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 20\n",
    "\n",
    "trainer = TrainUtil(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0,\n",
    "                    handcrafted=True)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a228b9-ed3d-42d1-a653-7835d50802f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8758301d-c159-48f0-bad2-5e053417bca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "51a979b6-c3b2-467b-8c2f-e0c627ce6d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset load successfully!\n",
      "torch.Size([30, 17]) torch.Size([34])\n"
     ]
    }
   ],
   "source": [
    "MAX_RUL = 130\n",
    "trainset = CMAPSSDatasetHC(mode='train',\n",
    "                               data_path='./CMAPSSData/train_FD001.txt', max_rul=MAX_RUL)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CMAPSSDatasetHC(mode='test',\n",
    "                              data_path='./CMAPSSData/test_FD001.txt',\n",
    "                              rul_path='./CMAPSSData/RUL_FD001.txt',  max_rul=MAX_RUL)\n",
    "test_loader = DataLoader(dataset=testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print('dataset load successfully!')\n",
    "print(next(iter(trainset))[0].shape, next(iter(trainset))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "afed92d2-cc19-42c6-92e2-eabe245a0c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17731, 30, 17)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "21ffa731-d2ad-4c23-b0a0-61fa76b81236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 30, 17)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ee18a-5f02-4b7c-bf1e-984e74742156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fc874a41-99ee-4130-b4bc-2805b373a081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "output shape: [100, 30, 32] hidden shape: [2, 100, 32]\n",
      "Epoch: 1 train loss: 77.205 val loss: 69.426 score: 1070242.781\n",
      "Epoch: 2 train loss: 76.381 val loss: 68.564 score: 962208.812\n",
      "Epoch: 3 train loss: 75.465 val loss: 67.602 score: 853579.750\n",
      "Epoch: 4 train loss: 74.394 val loss: 66.494 score: 742736.125\n",
      "Epoch: 5 train loss: 73.321 val loss: 65.408 score: 647193.250\n",
      "Epoch: 6 train loss: 72.391 val loss: 64.273 score: 559238.828\n",
      "Epoch: 7 train loss: 71.427 val loss: 63.262 score: 490472.938\n",
      "Epoch: 8 train loss: 70.201 val loss: 62.019 score: 416701.797\n",
      "Epoch: 9 train loss: 68.404 val loss: 60.327 score: 332457.266\n",
      "Epoch: 10 train loss: 66.186 val loss: 58.243 score: 249666.172\n",
      "Epoch: 11 train loss: 63.788 val loss: 55.859 score: 177549.102\n",
      "Epoch: 12 train loss: 61.144 val loss: 53.258 score: 119831.699\n",
      "Epoch: 13 train loss: 58.355 val loss: 50.574 score: 77342.082\n",
      "Epoch: 14 train loss: 55.381 val loss: 47.939 score: 47946.516\n",
      "Epoch: 15 train loss: 53.090 val loss: 45.664 score: 29816.470\n",
      "Epoch: 16 train loss: 51.028 val loss: 43.903 score: 19262.371\n",
      "Epoch: 17 train loss: 49.537 val loss: 42.689 score: 13384.702\n",
      "Epoch: 18 train loss: 48.649 val loss: 41.963 score: 10330.289\n",
      "Epoch: 19 train loss: 47.981 val loss: 41.548 score: 8771.365\n",
      "Epoch: 20 train loss: 47.812 val loss: 41.288 score: 7937.039\n",
      "Epoch: 21 train loss: 47.397 val loss: 41.102 score: 7486.930\n",
      "Epoch: 22 train loss: 47.572 val loss: 40.946 score: 7227.567\n",
      "Epoch: 23 train loss: 47.206 val loss: 40.804 score: 7080.422\n",
      "Epoch: 24 train loss: 47.034 val loss: 40.652 score: 6888.381\n",
      "Epoch: 25 train loss: 46.776 val loss: 40.499 score: 6738.143\n",
      "Epoch: 26 train loss: 46.314 val loss: 40.340 score: 6583.523\n",
      "Epoch: 27 train loss: 46.266 val loss: 40.187 score: 6507.031\n",
      "Epoch: 28 train loss: 46.525 val loss: 40.025 score: 6416.445\n",
      "Epoch: 29 train loss: 45.950 val loss: 39.847 score: 6291.493\n",
      "Epoch: 30 train loss: 45.679 val loss: 39.668 score: 6211.870\n",
      "Epoch: 31 train loss: 45.514 val loss: 39.475 score: 6108.957\n",
      "Epoch: 32 train loss: 45.203 val loss: 39.266 score: 5991.504\n",
      "Epoch: 33 train loss: 44.924 val loss: 39.042 score: 5869.471\n",
      "Epoch: 34 train loss: 44.813 val loss: 38.830 score: 5835.985\n",
      "Epoch: 35 train loss: 44.616 val loss: 38.587 score: 5733.672\n",
      "Epoch: 36 train loss: 44.090 val loss: 38.346 score: 5681.591\n",
      "Epoch: 37 train loss: 43.799 val loss: 38.097 score: 5646.381\n",
      "Epoch: 38 train loss: 43.755 val loss: 37.829 score: 5586.201\n",
      "Epoch: 39 train loss: 43.376 val loss: 37.544 score: 5527.098\n",
      "Epoch: 40 train loss: 43.020 val loss: 37.229 score: 5432.125\n",
      "Epoch: 41 train loss: 42.584 val loss: 36.952 score: 5450.126\n",
      "Epoch: 42 train loss: 42.215 val loss: 36.696 score: 5533.661\n",
      "Epoch: 43 train loss: 41.783 val loss: 36.440 score: 5629.551\n",
      "Epoch: 44 train loss: 41.224 val loss: 36.103 score: 5604.083\n",
      "Epoch: 45 train loss: 40.865 val loss: 35.899 score: 5823.068\n",
      "Epoch: 46 train loss: 40.380 val loss: 35.690 score: 6050.440\n",
      "Epoch: 47 train loss: 39.607 val loss: 35.398 score: 6159.583\n",
      "Epoch: 48 train loss: 39.267 val loss: 35.278 score: 6561.946\n",
      "Epoch: 49 train loss: 38.785 val loss: 35.100 score: 6890.304\n",
      "Epoch: 50 train loss: 38.278 val loss: 34.989 score: 7339.206\n",
      "Epoch: 51 train loss: 37.696 val loss: 35.090 score: 8152.995\n",
      "Epoch: 52 train loss: 37.176 val loss: 35.003 score: 8708.676\n",
      "Epoch: 53 train loss: 36.831 val loss: 35.243 score: 9832.338\n",
      "Epoch: 54 train loss: 36.495 val loss: 35.351 score: 10792.235\n",
      "Epoch: 55 train loss: 35.953 val loss: 35.751 score: 12352.721\n",
      "Epoch: 56 train loss: 35.254 val loss: 35.861 score: 13424.833\n",
      "Epoch: 57 train loss: 34.985 val loss: 36.203 score: 14995.429\n",
      "Epoch: 58 train loss: 34.464 val loss: 37.027 score: 18171.688\n",
      "Epoch: 59 train loss: 34.145 val loss: 37.443 score: 20265.095\n",
      "Epoch: 60 train loss: 33.994 val loss: 37.814 score: 22309.549\n"
     ]
    }
   ],
   "source": [
    "model = LSTMBaseline(n_features=17, n_hidden=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
    "epochs = 60\n",
    "\n",
    "trainer = TrainUtilHC(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ba4af0ca-2730-45ab-8e71-60bfbd86851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "output shape: 100 30 50 hidden shape: 2 100 50\n",
      "\n",
      "Epoch: 1 train loss: 74.438 val loss: 62.626 score: 1887591.844\n",
      "Epoch: 2 train loss: 70.535 val loss: 54.540 score: 402725.047\n",
      "Epoch: 3 train loss: 56.740 val loss: 49.532 score: 50010.076\n",
      "Epoch: 4 train loss: 51.770 val loss: 51.122 score: 44304.367\n",
      "Epoch: 5 train loss: 51.382 val loss: 51.263 score: 45131.449\n",
      "Epoch: 6 train loss: 51.226 val loss: 51.290 score: 45298.317\n",
      "Epoch: 7 train loss: 51.180 val loss: 51.321 score: 45489.186\n",
      "Epoch: 8 train loss: 51.136 val loss: 51.347 score: 45652.155\n",
      "Epoch: 9 train loss: 51.078 val loss: 51.352 score: 45687.467\n",
      "Epoch: 10 train loss: 51.014 val loss: 51.321 score: 45491.335\n",
      "Epoch: 11 train loss: 51.008 val loss: 51.228 score: 44919.338\n",
      "Epoch: 12 train loss: 50.913 val loss: 51.237 score: 44971.194\n",
      "Epoch: 13 train loss: 50.967 val loss: 51.192 score: 44706.427\n",
      "Epoch: 14 train loss: 50.904 val loss: 51.379 score: 45858.915\n",
      "Epoch: 15 train loss: 50.912 val loss: 51.204 score: 44773.965\n",
      "Epoch: 16 train loss: 50.866 val loss: 51.116 score: 44268.501\n",
      "Epoch: 17 train loss: 50.926 val loss: 51.286 score: 45271.010\n",
      "Epoch: 18 train loss: 50.863 val loss: 51.204 score: 44778.481\n",
      "Epoch: 19 train loss: 50.852 val loss: 51.114 score: 44260.254\n",
      "Epoch: 20 train loss: 50.859 val loss: 51.108 score: 44224.348\n"
     ]
    }
   ],
   "source": [
    "model = LSTMBaselineAtn(n_features=17, n_hidden=50, time_steps=30)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "epochs = 20\n",
    "\n",
    "trainer = TrainUtilHC(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f6af5bc1-5e69-437b-8d0f-a3f25a8cd840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch: 1 train loss: 102.100 val loss: 91.346 score: 30705721.000\n",
      "Epoch: 2 train loss: 99.609 val loss: 88.976 score: 19733672.000\n",
      "Epoch: 3 train loss: 96.735 val loss: 85.774 score: 11791595.250\n",
      "Epoch: 4 train loss: 93.061 val loss: 81.243 score: 6189282.750\n",
      "Epoch: 5 train loss: 87.411 val loss: 74.190 score: 2485348.750\n",
      "Epoch: 6 train loss: 77.470 val loss: 61.318 score: 497107.562\n",
      "Epoch: 7 train loss: 65.036 val loss: 49.634 score: 102145.559\n",
      "Epoch: 8 train loss: 54.394 val loss: 40.299 score: 25159.545\n",
      "Epoch: 9 train loss: 46.150 val loss: 33.690 score: 7725.400\n",
      "Epoch: 10 train loss: 40.263 val loss: 29.873 score: 3260.595\n",
      "Epoch: 11 train loss: 36.711 val loss: 27.956 score: 1943.964\n",
      "Epoch: 12 train loss: 34.821 val loss: 26.915 score: 1481.059\n",
      "Epoch: 13 train loss: 33.228 val loss: 26.081 score: 1260.547\n",
      "Epoch: 14 train loss: 32.477 val loss: 25.303 score: 1116.995\n",
      "Epoch: 15 train loss: 31.501 val loss: 24.543 score: 1005.788\n",
      "Epoch: 16 train loss: 30.801 val loss: 23.774 score: 919.359\n",
      "Epoch: 17 train loss: 30.307 val loss: 23.108 score: 849.239\n",
      "Epoch: 18 train loss: 29.954 val loss: 22.589 score: 792.255\n",
      "Epoch: 19 train loss: 29.454 val loss: 22.069 score: 747.569\n",
      "Epoch: 20 train loss: 28.984 val loss: 21.668 score: 711.144\n",
      "Epoch: 21 train loss: 28.559 val loss: 21.284 score: 682.427\n",
      "Epoch: 22 train loss: 28.363 val loss: 20.956 score: 660.239\n",
      "Epoch: 23 train loss: 27.963 val loss: 20.742 score: 641.214\n",
      "Epoch: 24 train loss: 27.932 val loss: 20.548 score: 626.160\n",
      "Epoch: 25 train loss: 27.445 val loss: 20.364 score: 614.513\n",
      "Epoch: 26 train loss: 27.553 val loss: 20.285 score: 601.911\n",
      "Epoch: 27 train loss: 27.254 val loss: 20.137 score: 591.992\n",
      "Epoch: 28 train loss: 27.191 val loss: 20.051 score: 582.498\n",
      "Epoch: 29 train loss: 26.630 val loss: 19.910 score: 574.138\n",
      "Epoch: 30 train loss: 26.682 val loss: 19.644 score: 566.178\n",
      "Epoch: 31 train loss: 26.477 val loss: 19.506 score: 557.097\n",
      "Epoch: 32 train loss: 26.221 val loss: 19.363 score: 548.920\n",
      "Epoch: 33 train loss: 26.185 val loss: 19.255 score: 540.268\n",
      "Epoch: 34 train loss: 25.985 val loss: 19.045 score: 533.289\n",
      "Epoch: 35 train loss: 25.920 val loss: 18.911 score: 526.426\n",
      "Epoch: 36 train loss: 25.728 val loss: 18.694 score: 520.653\n",
      "Epoch: 37 train loss: 25.505 val loss: 18.584 score: 513.470\n",
      "Epoch: 38 train loss: 25.210 val loss: 18.458 score: 507.783\n",
      "Epoch: 39 train loss: 25.024 val loss: 18.322 score: 503.528\n",
      "Epoch: 40 train loss: 24.827 val loss: 18.120 score: 502.328\n"
     ]
    }
   ],
   "source": [
    "model = LSTMHCAtn()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "epochs = 40\n",
    "\n",
    "trainer = TrainUtilHC(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d3684-2e55-43ed-a1e6-cefedb1fb0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dde800d-c9d2-4c7f-b2ae-de3b504e9c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306be2a-a934-4cd5-9053-f2d113ebb8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef20ff3d-925e-4a77-a16d-f7a300bfde51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):   \n",
    "    def __init__(self, X, y, seq_len=1):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index:index+self.seq_len], self.y[index+self.seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7255a3db-8a64-4592-9d28-1dc9fff65c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        train_df,\n",
    "        test_df,\n",
    "        label_name,\n",
    "        sequence_length,\n",
    "        batch_size,\n",
    "        n_epochs,\n",
    "        n_epochs_stop,\n",
    "        lr\n",
    "):\n",
    "    \"\"\"Train LSTM model.\"\"\"\n",
    "    print(\"Starting with model training...\")\n",
    "\n",
    "    # create dataloaders\n",
    "    train_dataset = TimeSeriesDataset(np.array(train_df), np.array(train_df[label_name]), seq_len=sequence_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_dataset = TimeSeriesDataset(np.array(test_df), np.array(test_df[label_name]), seq_len=sequence_length)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "    # set up training\n",
    "    #n_features = train_df.shape[1]\n",
    "    #model = TSModel(n_features)\n",
    "    criterion = torch.nn.MSELoss()  # L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_hist = []\n",
    "    test_hist = []\n",
    "\n",
    "    # start training\n",
    "    best_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            data = torch.Tensor(np.array(data))\n",
    "            \n",
    "            if (epoch == 1) and (batch_idx == 1):\n",
    "                print(\"input shape:\", *data.shape)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            if (epoch == 1) and (batch_idx == 1):\n",
    "                print(\"model output:\", *output.shape)\n",
    "                print()\n",
    "            \n",
    "            loss = criterion(output.flatten(), target.type_as(output))\n",
    "            # if type(criterion) == torch.nn.modules.loss.MSELoss:\n",
    "            #     loss = torch.sqrt(loss)  # RMSE\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        running_loss /= len(train_loader)\n",
    "        train_hist.append(running_loss)\n",
    "\n",
    "        # test loss\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data = torch.Tensor(np.array(data))\n",
    "                output = model(data)\n",
    "                loss = criterion(output.flatten(), target.type_as(output))\n",
    "                test_loss += loss.item()\n",
    "            test_loss /= len(test_loader)\n",
    "            test_hist.append(test_loss)\n",
    "\n",
    "            # early stopping\n",
    "            if test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                torch.save(model.state_dict(), Path(model_dir, 'model.pt'))\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            if epochs_no_improve == n_epochs_stop:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch} train loss: {round(running_loss,4)} test loss: {round(test_loss,4)}')\n",
    "\n",
    "        hist = pd.DataFrame()\n",
    "        hist['training_loss'] = train_hist\n",
    "        hist['test_loss'] = test_hist\n",
    "\n",
    "    print(\"Completed.\")\n",
    "\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "900da2b7-5b1a-403c-ab46-efc2b3b3eece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High_Low_Pct</th>\n",
       "      <th>Open_Close_Pct</th>\n",
       "      <th>Day_Of_Week</th>\n",
       "      <th>Month_Of_Year</th>\n",
       "      <th>Quarter_Of_Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>0.081764</td>\n",
       "      <td>0.193811</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006946</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>0.047454</td>\n",
       "      <td>0.192291</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009084</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012580</td>\n",
       "      <td>0.023945</td>\n",
       "      <td>0.019690</td>\n",
       "      <td>0.191727</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051411</td>\n",
       "      <td>0.077130</td>\n",
       "      <td>0.239106</td>\n",
       "      <td>0.186352</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.654109</td>\n",
       "      <td>0.044822</td>\n",
       "      <td>0.048717</td>\n",
       "      <td>0.171630</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.650104</td>\n",
       "      <td>0.029176</td>\n",
       "      <td>0.084043</td>\n",
       "      <td>0.159411</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.592014</td>\n",
       "      <td>0.032559</td>\n",
       "      <td>0.141697</td>\n",
       "      <td>0.171933</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.583220</td>\n",
       "      <td>0.036661</td>\n",
       "      <td>0.119429</td>\n",
       "      <td>0.201692</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.626679</td>\n",
       "      <td>0.021036</td>\n",
       "      <td>0.094614</td>\n",
       "      <td>0.184985</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Close    Volume  High_Low_Pct  Open_Close_Pct  Day_Of_Week  \\\n",
       "0    0.000000  0.008925      0.081764        0.193811     0.833333   \n",
       "1    0.006946  0.020057      0.047454        0.192291     1.000000   \n",
       "2    0.009084  0.011096      0.000000        0.189650     0.000000   \n",
       "3    0.012580  0.023945      0.019690        0.191727     0.166667   \n",
       "4    0.051411  0.077130      0.239106        0.186352     0.333333   \n",
       "..        ...       ...           ...             ...          ...   \n",
       "360  0.654109  0.044822      0.048717        0.171630     0.166667   \n",
       "361  0.650104  0.029176      0.084043        0.159411     0.333333   \n",
       "362  0.592014  0.032559      0.141697        0.171933     0.500000   \n",
       "363  0.583220  0.036661      0.119429        0.201692     0.666667   \n",
       "364  0.626679  0.021036      0.094614        0.184985     0.833333   \n",
       "\n",
       "     Month_Of_Year  Quarter_Of_Year  \n",
       "0              1.0              1.0  \n",
       "1              1.0              1.0  \n",
       "2              1.0              1.0  \n",
       "3              1.0              1.0  \n",
       "4              1.0              1.0  \n",
       "..             ...              ...  \n",
       "360            1.0              1.0  \n",
       "361            1.0              1.0  \n",
       "362            1.0              1.0  \n",
       "363            1.0              1.0  \n",
       "364            1.0              1.0  \n",
       "\n",
       "[365 rows x 7 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63c951a8-f8d0-46c4-8e96-a8924696c49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High_Low_Pct</th>\n",
       "      <th>Open_Close_Pct</th>\n",
       "      <th>Day_Of_Week</th>\n",
       "      <th>Month_Of_Year</th>\n",
       "      <th>Quarter_Of_Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.641769</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.063496</td>\n",
       "      <td>0.198277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.572847</td>\n",
       "      <td>0.040276</td>\n",
       "      <td>0.194433</td>\n",
       "      <td>0.172954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.570286</td>\n",
       "      <td>0.047717</td>\n",
       "      <td>0.068952</td>\n",
       "      <td>1.233623</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.617127</td>\n",
       "      <td>0.053446</td>\n",
       "      <td>0.110560</td>\n",
       "      <td>0.191364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.591876</td>\n",
       "      <td>0.025529</td>\n",
       "      <td>0.060670</td>\n",
       "      <td>0.182496</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>-0.040103</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.014960</td>\n",
       "      <td>0.187984</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>-0.032200</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.032581</td>\n",
       "      <td>0.187136</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>-0.034257</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>-0.012056</td>\n",
       "      <td>0.190973</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>-0.034348</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>-0.024171</td>\n",
       "      <td>0.190842</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>-0.034851</td>\n",
       "      <td>-0.014045</td>\n",
       "      <td>-0.015794</td>\n",
       "      <td>0.138219</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Close    Volume  High_Low_Pct  Open_Close_Pct  Day_Of_Week  \\\n",
       "0    0.641769  0.009487      0.063496        0.198277     1.000000   \n",
       "1    0.572847  0.040276      0.194433        0.172954     0.000000   \n",
       "2    0.570286  0.047717      0.068952        1.233623     0.166667   \n",
       "3    0.617127  0.053446      0.110560        0.191364     0.333333   \n",
       "4    0.591876  0.025529      0.060670        0.182496     0.500000   \n",
       "..        ...       ...           ...             ...          ...   \n",
       "360 -0.040103  0.002671      0.014960        0.187984     0.333333   \n",
       "361 -0.032200  0.005144      0.032581        0.187136     0.500000   \n",
       "362 -0.034257  0.004637     -0.012056        0.190973     0.666667   \n",
       "363 -0.034348 -0.018307     -0.024171        0.190842     0.833333   \n",
       "364 -0.034851 -0.014045     -0.015794        0.138219     1.000000   \n",
       "\n",
       "     Month_Of_Year  Quarter_Of_Year  \n",
       "0              1.0              1.0  \n",
       "1              1.0              1.0  \n",
       "2              1.0              1.0  \n",
       "3              1.0              1.0  \n",
       "4              1.0              1.0  \n",
       "..             ...              ...  \n",
       "360            1.0              1.0  \n",
       "361            1.0              1.0  \n",
       "362            1.0              1.0  \n",
       "363            1.0              1.0  \n",
       "364            1.0              1.0  \n",
       "\n",
       "[365 rows x 7 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39decdb3-52c2-4996-a694-9490c4244d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12-12</td>\n",
       "      <td>18051.320313</td>\n",
       "      <td>18919.550781</td>\n",
       "      <td>18046.041016</td>\n",
       "      <td>18803.656250</td>\n",
       "      <td>18803.656250</td>\n",
       "      <td>21752580802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-12-13</td>\n",
       "      <td>18806.765625</td>\n",
       "      <td>19381.535156</td>\n",
       "      <td>18734.332031</td>\n",
       "      <td>19142.382813</td>\n",
       "      <td>19142.382813</td>\n",
       "      <td>25450468637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-14</td>\n",
       "      <td>19144.492188</td>\n",
       "      <td>19305.099609</td>\n",
       "      <td>19012.708984</td>\n",
       "      <td>19246.644531</td>\n",
       "      <td>19246.644531</td>\n",
       "      <td>22473997681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-15</td>\n",
       "      <td>19246.919922</td>\n",
       "      <td>19525.007813</td>\n",
       "      <td>19079.841797</td>\n",
       "      <td>19417.076172</td>\n",
       "      <td>19417.076172</td>\n",
       "      <td>26741982541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>19418.818359</td>\n",
       "      <td>21458.908203</td>\n",
       "      <td>19298.316406</td>\n",
       "      <td>21310.597656</td>\n",
       "      <td>21310.597656</td>\n",
       "      <td>44409011479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>2022-12-08</td>\n",
       "      <td>16847.349609</td>\n",
       "      <td>17267.916016</td>\n",
       "      <td>16788.783203</td>\n",
       "      <td>17233.474609</td>\n",
       "      <td>17233.474609</td>\n",
       "      <td>20496603770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>2022-12-09</td>\n",
       "      <td>17232.148438</td>\n",
       "      <td>17280.546875</td>\n",
       "      <td>17100.835938</td>\n",
       "      <td>17133.152344</td>\n",
       "      <td>17133.152344</td>\n",
       "      <td>20328426366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>2022-12-10</td>\n",
       "      <td>17134.220703</td>\n",
       "      <td>17216.826172</td>\n",
       "      <td>17120.683594</td>\n",
       "      <td>17128.724609</td>\n",
       "      <td>17128.724609</td>\n",
       "      <td>12706781969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>2022-12-11</td>\n",
       "      <td>17129.710938</td>\n",
       "      <td>17245.634766</td>\n",
       "      <td>17091.820313</td>\n",
       "      <td>17104.193359</td>\n",
       "      <td>17104.193359</td>\n",
       "      <td>14122486832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>2022-12-12</td>\n",
       "      <td>17072.505859</td>\n",
       "      <td>17072.505859</td>\n",
       "      <td>16899.478516</td>\n",
       "      <td>16973.279297</td>\n",
       "      <td>16973.279297</td>\n",
       "      <td>18718484480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>731 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date          Open          High           Low         Close  \\\n",
       "0    2020-12-12  18051.320313  18919.550781  18046.041016  18803.656250   \n",
       "1    2020-12-13  18806.765625  19381.535156  18734.332031  19142.382813   \n",
       "2    2020-12-14  19144.492188  19305.099609  19012.708984  19246.644531   \n",
       "3    2020-12-15  19246.919922  19525.007813  19079.841797  19417.076172   \n",
       "4    2020-12-16  19418.818359  21458.908203  19298.316406  21310.597656   \n",
       "..          ...           ...           ...           ...           ...   \n",
       "726  2022-12-08  16847.349609  17267.916016  16788.783203  17233.474609   \n",
       "727  2022-12-09  17232.148438  17280.546875  17100.835938  17133.152344   \n",
       "728  2022-12-10  17134.220703  17216.826172  17120.683594  17128.724609   \n",
       "729  2022-12-11  17129.710938  17245.634766  17091.820313  17104.193359   \n",
       "730  2022-12-12  17072.505859  17072.505859  16899.478516  16973.279297   \n",
       "\n",
       "        Adj Close       Volume  \n",
       "0    18803.656250  21752580802  \n",
       "1    19142.382813  25450468637  \n",
       "2    19246.644531  22473997681  \n",
       "3    19417.076172  26741982541  \n",
       "4    21310.597656  44409011479  \n",
       "..            ...          ...  \n",
       "726  17233.474609  20496603770  \n",
       "727  17133.152344  20328426366  \n",
       "728  17128.724609  12706781969  \n",
       "729  17104.193359  14122486832  \n",
       "730  16973.279297  18718484480  \n",
       "\n",
       "[731 rows x 7 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/BTC-USD.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8beed2af-5c24-49c6-8ad7-0a29cef8e534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "668ea13f-557b-4143-b046-b8205cc2ab97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with model training...\n",
      "input shape: 10 30 7\n",
      "output shape: 10 30 64 hidden shape: 2 10 64\n",
      "model output: 10 1\n",
      "\n",
      "Epoch 1 train loss: 0.1495 test loss: 0.2826\n",
      "Epoch 2 train loss: 0.0552 test loss: 0.1772\n",
      "Epoch 3 train loss: 0.0611 test loss: 0.1177\n",
      "Epoch 4 train loss: 0.0655 test loss: 0.1097\n",
      "Epoch 5 train loss: 0.0517 test loss: 0.1274\n",
      "Epoch 6 train loss: 0.046 test loss: 0.1184\n",
      "Epoch 7 train loss: 0.0396 test loss: 0.1027\n",
      "Epoch 8 train loss: 0.0378 test loss: 0.0871\n",
      "Epoch 9 train loss: 0.0323 test loss: 0.0674\n",
      "Epoch 10 train loss: 0.0285 test loss: 0.04\n",
      "Epoch 11 train loss: 0.0244 test loss: 0.0245\n",
      "Epoch 12 train loss: 0.011 test loss: 0.0196\n",
      "Epoch 13 train loss: 0.0099 test loss: 0.0512\n",
      "Epoch 14 train loss: 0.0113 test loss: 0.0423\n",
      "Epoch 15 train loss: 0.006 test loss: 0.0314\n",
      "Epoch 16 train loss: 0.0057 test loss: 0.0288\n",
      "Epoch 17 train loss: 0.0055 test loss: 0.026\n",
      "Epoch 18 train loss: 0.0057 test loss: 0.022\n",
      "Epoch 19 train loss: 0.0053 test loss: 0.029\n",
      "Epoch 20 train loss: 0.0054 test loss: 0.0209\n",
      "Epoch 21 train loss: 0.0066 test loss: 0.0276\n",
      "Epoch 22 train loss: 0.0057 test loss: 0.0187\n",
      "Epoch 23 train loss: 0.0076 test loss: 0.0148\n",
      "Epoch 24 train loss: 0.0064 test loss: 0.0134\n",
      "Epoch 25 train loss: 0.0064 test loss: 0.0074\n",
      "Epoch 26 train loss: 0.0058 test loss: 0.0087\n",
      "Epoch 27 train loss: 0.0059 test loss: 0.0038\n",
      "Epoch 28 train loss: 0.0043 test loss: 0.0022\n",
      "Epoch 29 train loss: 0.0038 test loss: 0.0034\n",
      "Epoch 30 train loss: 0.0046 test loss: 0.0029\n",
      "Epoch 31 train loss: 0.0043 test loss: 0.0069\n",
      "Epoch 32 train loss: 0.0041 test loss: 0.0039\n",
      "Epoch 33 train loss: 0.004 test loss: 0.0058\n",
      "Epoch 34 train loss: 0.0038 test loss: 0.0048\n",
      "Epoch 35 train loss: 0.004 test loss: 0.0092\n",
      "Epoch 36 train loss: 0.0042 test loss: 0.0091\n",
      "Epoch 37 train loss: 0.0038 test loss: 0.008\n",
      "Epoch 38 train loss: 0.0038 test loss: 0.0069\n",
      "Epoch 39 train loss: 0.0036 test loss: 0.013\n",
      "Epoch 40 train loss: 0.0041 test loss: 0.0071\n",
      "Completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_loss</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.149510</td>\n",
       "      <td>0.282596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.055157</td>\n",
       "      <td>0.177239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061057</td>\n",
       "      <td>0.117689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.065541</td>\n",
       "      <td>0.109684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051653</td>\n",
       "      <td>0.127381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.045953</td>\n",
       "      <td>0.118389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.039573</td>\n",
       "      <td>0.102711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.037757</td>\n",
       "      <td>0.087150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.032257</td>\n",
       "      <td>0.067441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.028499</td>\n",
       "      <td>0.040018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.024376</td>\n",
       "      <td>0.024465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.010983</td>\n",
       "      <td>0.019571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.009861</td>\n",
       "      <td>0.051168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.042267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005986</td>\n",
       "      <td>0.031440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.005677</td>\n",
       "      <td>0.028820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.005456</td>\n",
       "      <td>0.025988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.005702</td>\n",
       "      <td>0.022026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.029029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.020896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.006589</td>\n",
       "      <td>0.027613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005724</td>\n",
       "      <td>0.018687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.007591</td>\n",
       "      <td>0.014827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.006359</td>\n",
       "      <td>0.013406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.006438</td>\n",
       "      <td>0.007429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.005798</td>\n",
       "      <td>0.008737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.005861</td>\n",
       "      <td>0.003849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.002233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.003431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.004557</td>\n",
       "      <td>0.002862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.006892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.004144</td>\n",
       "      <td>0.003939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.004042</td>\n",
       "      <td>0.005791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.004752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.009177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.004207</td>\n",
       "      <td>0.009082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.007966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.003782</td>\n",
       "      <td>0.006944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.013031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.007065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    training_loss  test_loss\n",
       "0        0.149510   0.282596\n",
       "1        0.055157   0.177239\n",
       "2        0.061057   0.117689\n",
       "3        0.065541   0.109684\n",
       "4        0.051653   0.127381\n",
       "5        0.045953   0.118389\n",
       "6        0.039573   0.102711\n",
       "7        0.037757   0.087150\n",
       "8        0.032257   0.067441\n",
       "9        0.028499   0.040018\n",
       "10       0.024376   0.024465\n",
       "11       0.010983   0.019571\n",
       "12       0.009861   0.051168\n",
       "13       0.011270   0.042267\n",
       "14       0.005986   0.031440\n",
       "15       0.005677   0.028820\n",
       "16       0.005456   0.025988\n",
       "17       0.005702   0.022026\n",
       "18       0.005346   0.029029\n",
       "19       0.005376   0.020896\n",
       "20       0.006589   0.027613\n",
       "21       0.005724   0.018687\n",
       "22       0.007591   0.014827\n",
       "23       0.006359   0.013406\n",
       "24       0.006438   0.007429\n",
       "25       0.005798   0.008737\n",
       "26       0.005861   0.003849\n",
       "27       0.004252   0.002233\n",
       "28       0.003816   0.003431\n",
       "29       0.004557   0.002862\n",
       "30       0.004303   0.006892\n",
       "31       0.004144   0.003939\n",
       "32       0.004042   0.005791\n",
       "33       0.003763   0.004752\n",
       "34       0.003959   0.009177\n",
       "35       0.004207   0.009082\n",
       "36       0.003774   0.007966\n",
       "37       0.003782   0.006944\n",
       "38       0.003597   0.013031\n",
       "39       0.004058   0.007065"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(\n",
    "        TSModel(df_train.shape[1]),\n",
    "        df_train,\n",
    "        df_test,\n",
    "        \"Close\",\n",
    "        sequence_length=30,\n",
    "        batch_size=10,\n",
    "        n_epochs=40,\n",
    "        n_epochs_stop=20,\n",
    "        lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bbe6204b-2470-457c-b15d-3bd90d6cbf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, time_steps=30):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(time_steps, time_steps),\n",
    "            #nn.Softmax(dim=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # inputs: batch size * window size(time step) * lstm output dims\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.permute(0, 2, 1)\n",
    "        x = self.linear(x)\n",
    "        x_probs = x.permute(0, 2, 1)\n",
    "        #print(\"probs\")\n",
    "        #print(x_probs)\n",
    "        #print()\n",
    "        # print(torch.sum(x_probs.item()))\n",
    "        output = x_probs * inputs\n",
    "        return output\n",
    "\n",
    "    \n",
    "class TSModelAttention(nn.Module):\n",
    "    def __init__(self, n_features, time_steps, n_hidden=64, n_layers=2):\n",
    "        super(TSModelAttention, self).__init__()\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.time_steps = time_steps\n",
    "        self.n_features = n_features\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_hidden,\n",
    "            batch_first=True,\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        self.attention = AttentionBlock(time_steps)\n",
    "        self.linear = nn.Linear(self.time_steps*self.n_hidden, 1)\n",
    "        self.printed = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, (hidden, _) = self.lstm(x)\n",
    "        if not self.printed:\n",
    "            print(\"output shape:\", *lstm_output.shape, \"hidden shape:\", *hidden.shape)\n",
    "            self.printed=True\n",
    "            print()\n",
    "        x = self.attention(lstm_output)\n",
    "        #print(x)\n",
    "        #print()\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1, self.time_steps*self.n_hidden)\n",
    "        #print(\"x shape\", x.shape)\n",
    "        lstm_out = hidden[-1]  # output last hidden state output\n",
    "        y_pred = self.linear(x)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5689af84-0be5-4f35-800b-37db9bff1e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with model training...\n",
      "input shape: 10 30 7\n",
      "output shape: 10 30 64 hidden shape: 1 10 64\n",
      "\n",
      "model output: 10 1\n",
      "\n",
      "Epoch 1 train loss: 0.137 test loss: 0.1445\n",
      "Epoch 2 train loss: 0.0605 test loss: 0.1355\n",
      "Epoch 3 train loss: 0.0612 test loss: 0.1225\n",
      "Epoch 4 train loss: 0.0504 test loss: 0.1176\n",
      "Epoch 5 train loss: 0.0432 test loss: 0.1059\n",
      "Epoch 6 train loss: 0.0393 test loss: 0.0937\n",
      "Epoch 7 train loss: 0.036 test loss: 0.0824\n",
      "Epoch 8 train loss: 0.032 test loss: 0.0712\n",
      "Epoch 9 train loss: 0.0274 test loss: 0.0597\n",
      "Epoch 10 train loss: 0.0226 test loss: 0.0484\n",
      "Epoch 11 train loss: 0.0178 test loss: 0.0376\n",
      "Epoch 12 train loss: 0.0134 test loss: 0.0284\n",
      "Epoch 13 train loss: 0.0101 test loss: 0.0243\n",
      "Epoch 14 train loss: 0.0082 test loss: 0.0267\n",
      "Epoch 15 train loss: 0.0076 test loss: 0.0312\n",
      "Epoch 16 train loss: 0.0073 test loss: 0.0344\n",
      "Epoch 17 train loss: 0.0071 test loss: 0.0353\n",
      "Epoch 18 train loss: 0.0073 test loss: 0.035\n",
      "Epoch 19 train loss: 0.0081 test loss: 0.0338\n",
      "Epoch 20 train loss: 0.0094 test loss: 0.0303\n",
      "Epoch 21 train loss: 0.0105 test loss: 0.0226\n",
      "Epoch 22 train loss: 0.0104 test loss: 0.0135\n",
      "Epoch 23 train loss: 0.0092 test loss: 0.0054\n",
      "Epoch 24 train loss: 0.0071 test loss: 0.0024\n",
      "Epoch 25 train loss: 0.0052 test loss: 0.0067\n",
      "Epoch 26 train loss: 0.0054 test loss: 0.0154\n",
      "Epoch 27 train loss: 0.006 test loss: 0.0252\n",
      "Epoch 28 train loss: 0.0061 test loss: 0.0334\n",
      "Epoch 29 train loss: 0.0056 test loss: 0.0336\n",
      "Epoch 30 train loss: 0.0055 test loss: 0.0315\n",
      "Completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_loss</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.137022</td>\n",
       "      <td>0.144499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060537</td>\n",
       "      <td>0.135549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.122494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050381</td>\n",
       "      <td>0.117582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.043236</td>\n",
       "      <td>0.105892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.039307</td>\n",
       "      <td>0.093682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.082431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.032012</td>\n",
       "      <td>0.071198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.027418</td>\n",
       "      <td>0.059735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.022584</td>\n",
       "      <td>0.048439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.017793</td>\n",
       "      <td>0.037609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.013441</td>\n",
       "      <td>0.028420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.024293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.008158</td>\n",
       "      <td>0.026747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.007568</td>\n",
       "      <td>0.031176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.007269</td>\n",
       "      <td>0.034354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.007065</td>\n",
       "      <td>0.035335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.007263</td>\n",
       "      <td>0.034953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.008113</td>\n",
       "      <td>0.033783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.009412</td>\n",
       "      <td>0.030267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.010514</td>\n",
       "      <td>0.022622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.010398</td>\n",
       "      <td>0.013545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.009201</td>\n",
       "      <td>0.005398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.002446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.005416</td>\n",
       "      <td>0.015384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.025241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.006053</td>\n",
       "      <td>0.033447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.033606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.031540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    training_loss  test_loss\n",
       "0        0.137022   0.144499\n",
       "1        0.060537   0.135549\n",
       "2        0.061200   0.122494\n",
       "3        0.050381   0.117582\n",
       "4        0.043236   0.105892\n",
       "5        0.039307   0.093682\n",
       "6        0.035971   0.082431\n",
       "7        0.032012   0.071198\n",
       "8        0.027418   0.059735\n",
       "9        0.022584   0.048439\n",
       "10       0.017793   0.037609\n",
       "11       0.013441   0.028420\n",
       "12       0.010116   0.024293\n",
       "13       0.008158   0.026747\n",
       "14       0.007568   0.031176\n",
       "15       0.007269   0.034354\n",
       "16       0.007065   0.035335\n",
       "17       0.007263   0.034953\n",
       "18       0.008113   0.033783\n",
       "19       0.009412   0.030267\n",
       "20       0.010514   0.022622\n",
       "21       0.010398   0.013545\n",
       "22       0.009201   0.005398\n",
       "23       0.007060   0.002446\n",
       "24       0.005216   0.006700\n",
       "25       0.005416   0.015384\n",
       "26       0.006025   0.025241\n",
       "27       0.006053   0.033447\n",
       "28       0.005581   0.033606\n",
       "29       0.005466   0.031540"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(\n",
    "        TSModelAttention(df_train.shape[1],  30, n_hidden=64, n_layers=1),\n",
    "        df_train,\n",
    "        df_test,\n",
    "        \"Close\",\n",
    "        sequence_length=30,\n",
    "        batch_size=10,\n",
    "        n_epochs=30,\n",
    "        n_epochs_stop=20,\n",
    "        lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572dcec5-e1e2-4782-9304-41870506ce00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f66106-3d02-462c-bf36-89decc90d1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cda425-d669-404e-a289-da3dddc0c83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fccfbcc-4c64-48f1-8259-392216572202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing: class X\n",
      "Initializing: class B\n",
      "Initializing: class A\n",
      "sub_method from class X: 1\n",
      "sub_method from class B: 2\n",
      "sub_method from class A: 3\n",
      "Обратите внимание как происходит инициализация\n",
      "классов при указании аргументов в функции super()\n",
      "Initializing: class Y\n",
      "Initializing: class X\n",
      "Initializing: class B\n",
      "Initializing: class A\n",
      "sub_method from class Y: 5\n",
      "sub_method from class X: 6\n",
      "sub_method from class B: 7\n",
      "sub_method from class A: 8\n"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self):\n",
    "        print('Initializing: class A')\n",
    "\n",
    "    def sub_method(self, b):\n",
    "        print('sub_method from class A:', b)\n",
    "\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self):\n",
    "        print('Initializing: class B')\n",
    "        super().__init__()\n",
    "\n",
    "    def sub_method(self, b):\n",
    "        print('sub_method from class B:', b)\n",
    "        super().sub_method(b + 1)\n",
    "\n",
    "class X(B):\n",
    "    def __init__(self):\n",
    "        print('Initializing: class X')\n",
    "        super().__init__()\n",
    "\n",
    "    def sub_method(self, b):\n",
    "        print('sub_method from class X:', b)\n",
    "        super().sub_method(b + 1)\n",
    "\n",
    "\n",
    "class Y(X):\n",
    "    def __init__(self):\n",
    "        print('Initializing: class Y')\n",
    "        # super() с параметрами\n",
    "        super(Y, self).__init__()\n",
    "\n",
    "    def sub_method(self, b):\n",
    "        print('sub_method from class Y:', b)\n",
    "        super().sub_method(b + 1)\n",
    "\n",
    "\n",
    "x = X()\n",
    "x.sub_method(1)\n",
    "print('Обратите внимание как происходит инициализация')\n",
    "print('классов при указании аргументов в функции super()')\n",
    "y = Y()\n",
    "y.sub_method(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ad1c20b-87ee-4f3f-b245-df556ed4f9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing: class X\n",
      "Initializing: class B\n",
      "Initializing: class A\n",
      "sub_method from class X: 1\n",
      "sub_method from class B: 2\n",
      "sub_method from class A: 3\n",
      "Обратите внимание как происходит инициализация\n",
      "классов при указании аргументов в функции super()\n",
      "Initializing: class Y\n",
      "Initializing: class B\n",
      "Initializing: class A\n",
      "sub_method from class Y: 5\n",
      "sub_method from class X: 6\n",
      "sub_method from class B: 7\n",
      "sub_method from class A: 8\n"
     ]
    }
   ],
   "source": [
    "class Y(X):\n",
    "    def __init__(self):\n",
    "        print('Initializing: class Y')\n",
    "        # super() с параметрами\n",
    "        super(X, self).__init__()\n",
    "\n",
    "    def sub_method(self, b):\n",
    "        print('sub_method from class Y:', b)\n",
    "        super().sub_method(b + 1)\n",
    "\n",
    "\n",
    "x = X()\n",
    "x.sub_method(1)\n",
    "print('Обратите внимание как происходит инициализация')\n",
    "print('классов при указании аргументов в функции super()')\n",
    "y = Y()\n",
    "y.sub_method(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "386cda73-424d-4800-a432-ae05224685a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2600,  0.4525, -1.4906],\n",
      "         [ 0.0912,  2.4932,  0.2790]],\n",
      "\n",
      "        [[ 0.0041,  0.0553,  0.6910],\n",
      "         [-0.0967,  0.6487,  0.2092]]])\n",
      "torch.Size([2, 2, 3])\n",
      "\n",
      "tensor([[[ 0.2600,  0.0912],\n",
      "         [ 0.4525,  2.4932],\n",
      "         [-1.4906,  0.2790]],\n",
      "\n",
      "        [[ 0.0041, -0.0967],\n",
      "         [ 0.0553,  0.6487],\n",
      "         [ 0.6910,  0.2092]]])\n",
      "\n",
      "tensor([[[ 0.2600,  0.0912],\n",
      "         [ 0.4525,  2.4932],\n",
      "         [-1.4906,  0.2790]],\n",
      "\n",
      "        [[ 0.0041, -0.0967],\n",
      "         [ 0.0553,  0.6487],\n",
      "         [ 0.6910,  0.2092]]])\n",
      "\n",
      "tensor([[[ 0.2600,  0.0912],\n",
      "         [ 0.4525,  2.4932],\n",
      "         [-1.4906,  0.2790]],\n",
      "\n",
      "        [[ 0.0041, -0.0967],\n",
      "         [ 0.0553,  0.6487],\n",
      "         [ 0.6910,  0.2092]]])\n",
      "\n",
      "tensor([[[ 0.2600,  0.4525, -1.4906],\n",
      "         [ 0.0041,  0.0553,  0.6910]],\n",
      "\n",
      "        [[ 0.0912,  2.4932,  0.2790],\n",
      "         [-0.0967,  0.6487,  0.2092]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2, 3)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "print()\n",
    "print(torch.transpose(x, 1, 2))\n",
    "print()\n",
    "print(torch.transpose(x,-1, -2))\n",
    "print()\n",
    "print(torch.transpose(x,-2, -1))\n",
    "print()\n",
    "print(torch.transpose(x,0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c2628-7779-487e-8d96-785b6d0946f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_dl] *",
   "language": "python",
   "name": "conda-env-ml_dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

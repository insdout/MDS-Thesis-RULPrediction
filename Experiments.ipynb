{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9af604ef-ffc1-4420-867e-1d89cd0bc49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from preprocess import CMAPSSSlidingWin\n",
    "from train import TrainUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c06888-21ba-49a5-a92f-94467a9a08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RUL=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f8d1b72-1434-4a96-a4b4-06ec420af517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMHCAtn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMHCAtn, self).__init__()\n",
    "        self.lstm = nn.LSTM(batch_first=True, input_size=17, hidden_size=50, num_layers=1)\n",
    "        self.attenion = Attention3dBlock()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=1500, out_features=50),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=50, out_features=10),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.handcrafted = nn.Sequential(\n",
    "            nn.Linear(in_features=34, out_features=10),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(in_features=20, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, handcrafted_feature):\n",
    "        y = self.handcrafted(handcrafted_feature)\n",
    "        x, (hn, cn) = self.lstm(inputs)\n",
    "        x = self.attenion(x)\n",
    "        # flatten\n",
    "        x = x.reshape(-1, 1500)\n",
    "        x = self.linear(x)\n",
    "        out = torch.concat((x, y), dim=1)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Attention3dBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention3dBlock, self).__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=30, out_features=30),\n",
    "            nn.Softmax(dim=2),\n",
    "        )\n",
    "\n",
    "    # inputs: batch size * window size(time step) * lstm output dims\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.permute(0, 2, 1)\n",
    "        x = self.linear(x)\n",
    "        x_probs = x.permute(0, 2, 1)\n",
    "        # print(torch.sum(x_probs.item()))\n",
    "        output = x_probs * inputs\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89ac799-8fb3-40eb-9c19-85cabd13af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBaseline(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden=64, n_layers=2):\n",
    "        super(LSTMBaseline, self).__init__()\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_hidden,\n",
    "            batch_first=True,\n",
    "            num_layers=n_layers,\n",
    " \n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=30*self.n_hidden, out_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=8, out_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=8, out_features=1)\n",
    "        )\n",
    "\n",
    "        self.printed = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, (hidden, _) = self.lstm(x)\n",
    "        if not self.printed:\n",
    "            print(\"output shape:\", list(lstm_output.shape), \"hidden shape:\", list(hidden.shape))\n",
    "            self.printed=True\n",
    "        lstm_out = lstm_output.reshape(-1, self.n_hidden*30)  # output last hidden state output\n",
    "        y_pred = self.linear(lstm_out)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaadcf8d-6950-427a-8325-5e6ec3d9b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, time_steps=30):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(time_steps, time_steps),\n",
    "            nn.Softmax(dim=2),\n",
    "            ##nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # inputs: batch size * window size(time step) * lstm output dims\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.permute(0, 2, 1)\n",
    "        x = self.linear(x)\n",
    "        x_probs = x.permute(0, 2, 1)\n",
    "        #print(\"probs\")\n",
    "        #print(x_probs)\n",
    "        #print()\n",
    "        # print(torch.sum(x_probs.item()))\n",
    "        output = x_probs * inputs\n",
    "        return output\n",
    "\n",
    "    \n",
    "class LSTMBaselineAtn(nn.Module):\n",
    "    def __init__(self, n_features, time_steps, n_hidden=64, n_layers=2):\n",
    "        super(LSTMBaselineAtn, self).__init__()\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.time_steps = time_steps\n",
    "        self.n_features = n_features\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_hidden,\n",
    "            batch_first=True,\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        self.attention = AttentionBlock(time_steps)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=self.time_steps*self.n_hidden, out_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=8, out_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=8, out_features=1)\n",
    "        )\n",
    "        self.printed = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, (hidden, _) = self.lstm(x)\n",
    "        if not self.printed:\n",
    "            print(\"output shape:\", *lstm_output.shape, \"hidden shape:\", *hidden.shape)\n",
    "            self.printed=True\n",
    "            print()\n",
    "        x = self.attention(lstm_output)\n",
    "        #print(x)\n",
    "        #print()\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1, self.time_steps*self.n_hidden)\n",
    "        #print(\"x shape\", x.shape)\n",
    "        lstm_out = hidden[-1]  # output last hidden state output\n",
    "        y_pred = self.linear(x)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d206a1-3297-4ca3-89c0-c4c19e2a8fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset load successfully!\n",
      "torch.Size([30, 17]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "trainset = CMAPSSSlidingWin(mode='train',\n",
    "                               data_path='./CMAPSSData/train_FD004.txt', max_rul=MAX_RUL)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CMAPSSSlidingWin(mode='test',\n",
    "                              data_path='./CMAPSSData/test_FD004.txt',\n",
    "                              rul_path='./CMAPSSData/RUL_FD004.txt',  max_rul=MAX_RUL)\n",
    "test_loader = DataLoader(dataset=testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print('dataset load successfully!')\n",
    "print(next(iter(trainset))[0].shape, next(iter(trainset))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f64dfc62-7d4a-481f-85a2-1fe489a2e0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54028,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3d1dbd-d2f5-4110-b451-c0cb7c0bc206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 30, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0688dc72-7ee2-4ad5-8320-3399445b2853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "output shape: [100, 30, 32] hidden shape: [2, 100, 32]\n",
      "Epoch: 1 train loss: 80.316 val loss: 65.786 score: 3105039.688\n",
      "Epoch: 2 train loss: 71.161 val loss: 55.719 score: 529188.508\n",
      "Epoch: 3 train loss: 60.608 val loss: 49.873 score: 84126.236\n",
      "Epoch: 4 train loss: 55.235 val loss: 49.975 score: 41513.540\n",
      "Epoch: 5 train loss: 54.358 val loss: 50.567 score: 42072.260\n",
      "Epoch: 6 train loss: 54.157 val loss: 50.587 score: 42106.857\n",
      "Epoch: 7 train loss: 54.101 val loss: 50.648 score: 42303.401\n",
      "Epoch: 8 train loss: 54.016 val loss: 50.573 score: 41973.726\n",
      "Epoch: 9 train loss: 53.834 val loss: 50.561 score: 41881.333\n",
      "Epoch: 10 train loss: 53.809 val loss: 50.497 score: 41604.235\n",
      "Epoch: 11 train loss: 53.716 val loss: 50.490 score: 41523.540\n",
      "Epoch: 12 train loss: 53.633 val loss: 50.483 score: 41442.923\n",
      "Epoch: 13 train loss: 53.327 val loss: 50.303 score: 40818.409\n",
      "Epoch: 14 train loss: 53.457 val loss: 50.300 score: 40734.299\n",
      "Epoch: 15 train loss: 53.266 val loss: 50.237 score: 40486.208\n",
      "Epoch: 16 train loss: 53.105 val loss: 50.318 score: 40632.169\n",
      "Epoch: 17 train loss: 53.118 val loss: 50.165 score: 40120.241\n",
      "Epoch: 18 train loss: 52.957 val loss: 50.156 score: 39994.695\n",
      "Epoch: 19 train loss: 52.833 val loss: 50.157 score: 39892.531\n",
      "Epoch: 20 train loss: 52.697 val loss: 49.942 score: 39286.001\n",
      "Epoch: 21 train loss: 52.509 val loss: 49.957 score: 39160.797\n",
      "Epoch: 22 train loss: 52.537 val loss: 49.900 score: 38892.285\n",
      "Epoch: 23 train loss: 52.276 val loss: 49.599 score: 38331.200\n",
      "Epoch: 24 train loss: 52.168 val loss: 49.566 score: 38024.123\n",
      "Epoch: 25 train loss: 51.990 val loss: 49.482 score: 37692.877\n",
      "Epoch: 26 train loss: 51.797 val loss: 49.287 score: 37339.843\n",
      "Epoch: 27 train loss: 51.527 val loss: 49.210 score: 36977.749\n",
      "Epoch: 28 train loss: 51.317 val loss: 48.895 score: 36809.045\n",
      "Epoch: 29 train loss: 51.115 val loss: 48.710 score: 36510.967\n",
      "Epoch: 30 train loss: 50.689 val loss: 48.425 score: 36483.417\n",
      "Epoch: 31 train loss: 50.470 val loss: 47.978 score: 38080.642\n",
      "Epoch: 32 train loss: 50.003 val loss: 47.710 score: 37473.598\n"
     ]
    }
   ],
   "source": [
    "model = LSTMBaseline(n_features=17, n_hidden=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
    "epochs = 32\n",
    "\n",
    "trainer = TrainUtil(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0)\n",
    "history = trainer.train(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed910a1e-2f03-4379-9d7d-b027b1b8bbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [45.78263541201871,\n",
       "  32.42722825831445,\n",
       "  30.66666656650857,\n",
       "  29.914496532249096,\n",
       "  29.422632867842005,\n",
       "  28.995562643621128,\n",
       "  28.75260589165381,\n",
       "  28.46332246092432,\n",
       "  28.20316802295496,\n",
       "  27.863100434878405,\n",
       "  27.8065462227606,\n",
       "  27.595207913354685,\n",
       "  27.394709008524956,\n",
       "  27.14947480219119,\n",
       "  27.119271047706388,\n",
       "  27.089987485191696,\n",
       "  26.999886460779166,\n",
       "  26.90291889299804,\n",
       "  26.91312721682428,\n",
       "  26.84738944982719,\n",
       "  26.75565819121115,\n",
       "  26.662291557319143,\n",
       "  26.597242827750698,\n",
       "  26.600093661541685,\n",
       "  26.490660304392517,\n",
       "  26.372250071696023,\n",
       "  26.241276536703946,\n",
       "  25.945287685403105,\n",
       "  25.776520199737966,\n",
       "  25.4642649281427,\n",
       "  25.145222307392185,\n",
       "  24.739649406902775],\n",
       " 'val_loss': [tensor(35.2570, device='cuda:0'),\n",
       "  tensor(32.2800, device='cuda:0'),\n",
       "  tensor(31.0330, device='cuda:0'),\n",
       "  tensor(31.3740, device='cuda:0'),\n",
       "  tensor(31.4569, device='cuda:0'),\n",
       "  tensor(30.8994, device='cuda:0'),\n",
       "  tensor(31.2649, device='cuda:0'),\n",
       "  tensor(30.6302, device='cuda:0'),\n",
       "  tensor(31.8174, device='cuda:0'),\n",
       "  tensor(30.3534, device='cuda:0'),\n",
       "  tensor(30.3932, device='cuda:0'),\n",
       "  tensor(30.4743, device='cuda:0'),\n",
       "  tensor(29.3157, device='cuda:0'),\n",
       "  tensor(30.1652, device='cuda:0'),\n",
       "  tensor(30.2503, device='cuda:0'),\n",
       "  tensor(29.4900, device='cuda:0'),\n",
       "  tensor(29.2307, device='cuda:0'),\n",
       "  tensor(29.0225, device='cuda:0'),\n",
       "  tensor(28.3859, device='cuda:0'),\n",
       "  tensor(29.0339, device='cuda:0'),\n",
       "  tensor(27.6876, device='cuda:0'),\n",
       "  tensor(27.9156, device='cuda:0'),\n",
       "  tensor(29.2254, device='cuda:0'),\n",
       "  tensor(27.7690, device='cuda:0'),\n",
       "  tensor(28.2822, device='cuda:0'),\n",
       "  tensor(27.8130, device='cuda:0'),\n",
       "  tensor(27.8515, device='cuda:0'),\n",
       "  tensor(27.2825, device='cuda:0'),\n",
       "  tensor(29.6470, device='cuda:0'),\n",
       "  tensor(28.3534, device='cuda:0'),\n",
       "  tensor(27.4548, device='cuda:0'),\n",
       "  tensor(27.4544, device='cuda:0')],\n",
       " 'val_score': [12791.6875,\n",
       "  8363.283813476562,\n",
       "  6974.557373046875,\n",
       "  6821.1439208984375,\n",
       "  7539.9847412109375,\n",
       "  6803.0078125,\n",
       "  6293.000427246094,\n",
       "  6007.4805908203125,\n",
       "  7570.173278808594,\n",
       "  5444.3486328125,\n",
       "  5673.794616699219,\n",
       "  5986.77294921875,\n",
       "  5545.536926269531,\n",
       "  5852.758117675781,\n",
       "  6627.006408691406,\n",
       "  5394.92919921875,\n",
       "  5332.361267089844,\n",
       "  4972.470642089844,\n",
       "  4894.7066650390625,\n",
       "  5220.139892578125,\n",
       "  4075.3851928710938,\n",
       "  4533.161437988281,\n",
       "  5879.058837890625,\n",
       "  4152.6912841796875,\n",
       "  4076.9027709960938,\n",
       "  4316.921081542969,\n",
       "  4838.625671386719,\n",
       "  3821.8260498046875,\n",
       "  4311.990173339844,\n",
       "  4109.451843261719,\n",
       "  3759.9392700195312,\n",
       "  4235.169006347656]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0455ce7-c1c6-4179-8f1e-e0b98d88c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset load successfully!\n",
      "torch.Size([30, 17]) torch.Size([34])\n"
     ]
    }
   ],
   "source": [
    "trainset = CMAPSSSlidingWin(mode='train',\n",
    "                               data_path='./CMAPSSData/train_FD004.txt', max_rul=MAX_RUL, handcrafted=True)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CMAPSSSlidingWin(mode='test',\n",
    "                              data_path='./CMAPSSData/test_FD004.txt',\n",
    "                              rul_path='./CMAPSSData/RUL_FD004.txt',  max_rul=MAX_RUL, handcrafted=True)\n",
    "test_loader = DataLoader(dataset=testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print('dataset load successfully!')\n",
    "print(next(iter(trainset))[0].shape, next(iter(trainset))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2976766-945b-45c1-8578-bfccccf25b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54028,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "782afaaf-59a2-40cb-b528-ba3d0a0c9076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 30, 17)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "353fc243-ec3c-4e0e-8565-c57dfa959509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 34)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.hc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4aab027-0720-4154-be71-31924cd33b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch: 1 train loss: 45.783 val loss: 35.257 score: 12791.688\n",
      "Epoch: 2 train loss: 32.427 val loss: 32.280 score: 8363.284\n",
      "Epoch: 3 train loss: 30.667 val loss: 31.033 score: 6974.557\n",
      "Epoch: 4 train loss: 29.914 val loss: 31.374 score: 6821.144\n",
      "Epoch: 5 train loss: 29.423 val loss: 31.457 score: 7539.985\n",
      "Epoch: 6 train loss: 28.996 val loss: 30.899 score: 6803.008\n",
      "Epoch: 7 train loss: 28.753 val loss: 31.265 score: 6293.000\n",
      "Epoch: 8 train loss: 28.463 val loss: 30.630 score: 6007.481\n",
      "Epoch: 9 train loss: 28.203 val loss: 31.817 score: 7570.173\n",
      "Epoch: 10 train loss: 27.863 val loss: 30.353 score: 5444.349\n",
      "Epoch: 11 train loss: 27.807 val loss: 30.393 score: 5673.795\n",
      "Epoch: 12 train loss: 27.595 val loss: 30.474 score: 5986.773\n",
      "Epoch: 13 train loss: 27.395 val loss: 29.316 score: 5545.537\n",
      "Epoch: 14 train loss: 27.149 val loss: 30.165 score: 5852.758\n",
      "Epoch: 15 train loss: 27.119 val loss: 30.250 score: 6627.006\n",
      "Epoch: 16 train loss: 27.090 val loss: 29.490 score: 5394.929\n",
      "Epoch: 17 train loss: 27.000 val loss: 29.231 score: 5332.361\n",
      "Epoch: 18 train loss: 26.903 val loss: 29.023 score: 4972.471\n",
      "Epoch: 19 train loss: 26.913 val loss: 28.386 score: 4894.707\n",
      "Epoch: 20 train loss: 26.847 val loss: 29.034 score: 5220.140\n",
      "Epoch: 21 train loss: 26.756 val loss: 27.688 score: 4075.385\n",
      "Epoch: 22 train loss: 26.662 val loss: 27.916 score: 4533.161\n",
      "Epoch: 23 train loss: 26.597 val loss: 29.225 score: 5879.059\n",
      "Epoch: 24 train loss: 26.600 val loss: 27.769 score: 4152.691\n",
      "Epoch: 25 train loss: 26.491 val loss: 28.282 score: 4076.903\n",
      "Epoch: 26 train loss: 26.372 val loss: 27.813 score: 4316.921\n",
      "Epoch: 27 train loss: 26.241 val loss: 27.852 score: 4838.626\n",
      "Epoch: 28 train loss: 25.945 val loss: 27.282 score: 3821.826\n",
      "Epoch: 29 train loss: 25.777 val loss: 29.647 score: 4311.990\n",
      "Epoch: 30 train loss: 25.464 val loss: 28.353 score: 4109.452\n",
      "Epoch: 31 train loss: 25.145 val loss: 27.455 score: 3759.939\n",
      "Epoch: 32 train loss: 24.740 val loss: 27.454 score: 4235.169\n"
     ]
    }
   ],
   "source": [
    "model = LSTMHCAtn()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 32\n",
    "\n",
    "trainer = TrainUtil(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0,\n",
    "                    handcrafted=True)\n",
    "history = trainer.train(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "51a979b6-c3b2-467b-8c2f-e0c627ce6d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset load successfully!\n",
      "torch.Size([30, 17]) torch.Size([34])\n"
     ]
    }
   ],
   "source": [
    "MAX_RUL = 130\n",
    "trainset = CMAPSSDatasetHC(mode='train',\n",
    "                               data_path='./CMAPSSData/train_FD001.txt', max_rul=MAX_RUL)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CMAPSSDatasetHC(mode='test',\n",
    "                              data_path='./CMAPSSData/test_FD001.txt',\n",
    "                              rul_path='./CMAPSSData/RUL_FD001.txt',  max_rul=MAX_RUL)\n",
    "test_loader = DataLoader(dataset=testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print('dataset load successfully!')\n",
    "print(next(iter(trainset))[0].shape, next(iter(trainset))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "afed92d2-cc19-42c6-92e2-eabe245a0c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17731, 30, 17)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "21ffa731-d2ad-4c23-b0a0-61fa76b81236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 30, 17)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ee18a-5f02-4b7c-bf1e-984e74742156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fc874a41-99ee-4130-b4bc-2805b373a081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "output shape: [100, 30, 32] hidden shape: [2, 100, 32]\n",
      "Epoch: 1 train loss: 77.205 val loss: 69.426 score: 1070242.781\n",
      "Epoch: 2 train loss: 76.381 val loss: 68.564 score: 962208.812\n",
      "Epoch: 3 train loss: 75.465 val loss: 67.602 score: 853579.750\n",
      "Epoch: 4 train loss: 74.394 val loss: 66.494 score: 742736.125\n",
      "Epoch: 5 train loss: 73.321 val loss: 65.408 score: 647193.250\n",
      "Epoch: 6 train loss: 72.391 val loss: 64.273 score: 559238.828\n",
      "Epoch: 7 train loss: 71.427 val loss: 63.262 score: 490472.938\n",
      "Epoch: 8 train loss: 70.201 val loss: 62.019 score: 416701.797\n",
      "Epoch: 9 train loss: 68.404 val loss: 60.327 score: 332457.266\n",
      "Epoch: 10 train loss: 66.186 val loss: 58.243 score: 249666.172\n",
      "Epoch: 11 train loss: 63.788 val loss: 55.859 score: 177549.102\n",
      "Epoch: 12 train loss: 61.144 val loss: 53.258 score: 119831.699\n",
      "Epoch: 13 train loss: 58.355 val loss: 50.574 score: 77342.082\n",
      "Epoch: 14 train loss: 55.381 val loss: 47.939 score: 47946.516\n",
      "Epoch: 15 train loss: 53.090 val loss: 45.664 score: 29816.470\n",
      "Epoch: 16 train loss: 51.028 val loss: 43.903 score: 19262.371\n",
      "Epoch: 17 train loss: 49.537 val loss: 42.689 score: 13384.702\n",
      "Epoch: 18 train loss: 48.649 val loss: 41.963 score: 10330.289\n",
      "Epoch: 19 train loss: 47.981 val loss: 41.548 score: 8771.365\n",
      "Epoch: 20 train loss: 47.812 val loss: 41.288 score: 7937.039\n",
      "Epoch: 21 train loss: 47.397 val loss: 41.102 score: 7486.930\n",
      "Epoch: 22 train loss: 47.572 val loss: 40.946 score: 7227.567\n",
      "Epoch: 23 train loss: 47.206 val loss: 40.804 score: 7080.422\n",
      "Epoch: 24 train loss: 47.034 val loss: 40.652 score: 6888.381\n",
      "Epoch: 25 train loss: 46.776 val loss: 40.499 score: 6738.143\n",
      "Epoch: 26 train loss: 46.314 val loss: 40.340 score: 6583.523\n",
      "Epoch: 27 train loss: 46.266 val loss: 40.187 score: 6507.031\n",
      "Epoch: 28 train loss: 46.525 val loss: 40.025 score: 6416.445\n",
      "Epoch: 29 train loss: 45.950 val loss: 39.847 score: 6291.493\n",
      "Epoch: 30 train loss: 45.679 val loss: 39.668 score: 6211.870\n",
      "Epoch: 31 train loss: 45.514 val loss: 39.475 score: 6108.957\n",
      "Epoch: 32 train loss: 45.203 val loss: 39.266 score: 5991.504\n",
      "Epoch: 33 train loss: 44.924 val loss: 39.042 score: 5869.471\n",
      "Epoch: 34 train loss: 44.813 val loss: 38.830 score: 5835.985\n",
      "Epoch: 35 train loss: 44.616 val loss: 38.587 score: 5733.672\n",
      "Epoch: 36 train loss: 44.090 val loss: 38.346 score: 5681.591\n",
      "Epoch: 37 train loss: 43.799 val loss: 38.097 score: 5646.381\n",
      "Epoch: 38 train loss: 43.755 val loss: 37.829 score: 5586.201\n",
      "Epoch: 39 train loss: 43.376 val loss: 37.544 score: 5527.098\n",
      "Epoch: 40 train loss: 43.020 val loss: 37.229 score: 5432.125\n",
      "Epoch: 41 train loss: 42.584 val loss: 36.952 score: 5450.126\n",
      "Epoch: 42 train loss: 42.215 val loss: 36.696 score: 5533.661\n",
      "Epoch: 43 train loss: 41.783 val loss: 36.440 score: 5629.551\n",
      "Epoch: 44 train loss: 41.224 val loss: 36.103 score: 5604.083\n",
      "Epoch: 45 train loss: 40.865 val loss: 35.899 score: 5823.068\n",
      "Epoch: 46 train loss: 40.380 val loss: 35.690 score: 6050.440\n",
      "Epoch: 47 train loss: 39.607 val loss: 35.398 score: 6159.583\n",
      "Epoch: 48 train loss: 39.267 val loss: 35.278 score: 6561.946\n",
      "Epoch: 49 train loss: 38.785 val loss: 35.100 score: 6890.304\n",
      "Epoch: 50 train loss: 38.278 val loss: 34.989 score: 7339.206\n",
      "Epoch: 51 train loss: 37.696 val loss: 35.090 score: 8152.995\n",
      "Epoch: 52 train loss: 37.176 val loss: 35.003 score: 8708.676\n",
      "Epoch: 53 train loss: 36.831 val loss: 35.243 score: 9832.338\n",
      "Epoch: 54 train loss: 36.495 val loss: 35.351 score: 10792.235\n",
      "Epoch: 55 train loss: 35.953 val loss: 35.751 score: 12352.721\n",
      "Epoch: 56 train loss: 35.254 val loss: 35.861 score: 13424.833\n",
      "Epoch: 57 train loss: 34.985 val loss: 36.203 score: 14995.429\n",
      "Epoch: 58 train loss: 34.464 val loss: 37.027 score: 18171.688\n",
      "Epoch: 59 train loss: 34.145 val loss: 37.443 score: 20265.095\n",
      "Epoch: 60 train loss: 33.994 val loss: 37.814 score: 22309.549\n"
     ]
    }
   ],
   "source": [
    "model = LSTMBaseline(n_features=17, n_hidden=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
    "epochs = 60\n",
    "\n",
    "trainer = TrainUtilHC(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ba4af0ca-2730-45ab-8e71-60bfbd86851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "output shape: 100 30 50 hidden shape: 2 100 50\n",
      "\n",
      "Epoch: 1 train loss: 74.438 val loss: 62.626 score: 1887591.844\n",
      "Epoch: 2 train loss: 70.535 val loss: 54.540 score: 402725.047\n",
      "Epoch: 3 train loss: 56.740 val loss: 49.532 score: 50010.076\n",
      "Epoch: 4 train loss: 51.770 val loss: 51.122 score: 44304.367\n",
      "Epoch: 5 train loss: 51.382 val loss: 51.263 score: 45131.449\n",
      "Epoch: 6 train loss: 51.226 val loss: 51.290 score: 45298.317\n",
      "Epoch: 7 train loss: 51.180 val loss: 51.321 score: 45489.186\n",
      "Epoch: 8 train loss: 51.136 val loss: 51.347 score: 45652.155\n",
      "Epoch: 9 train loss: 51.078 val loss: 51.352 score: 45687.467\n",
      "Epoch: 10 train loss: 51.014 val loss: 51.321 score: 45491.335\n",
      "Epoch: 11 train loss: 51.008 val loss: 51.228 score: 44919.338\n",
      "Epoch: 12 train loss: 50.913 val loss: 51.237 score: 44971.194\n",
      "Epoch: 13 train loss: 50.967 val loss: 51.192 score: 44706.427\n",
      "Epoch: 14 train loss: 50.904 val loss: 51.379 score: 45858.915\n",
      "Epoch: 15 train loss: 50.912 val loss: 51.204 score: 44773.965\n",
      "Epoch: 16 train loss: 50.866 val loss: 51.116 score: 44268.501\n",
      "Epoch: 17 train loss: 50.926 val loss: 51.286 score: 45271.010\n",
      "Epoch: 18 train loss: 50.863 val loss: 51.204 score: 44778.481\n",
      "Epoch: 19 train loss: 50.852 val loss: 51.114 score: 44260.254\n",
      "Epoch: 20 train loss: 50.859 val loss: 51.108 score: 44224.348\n"
     ]
    }
   ],
   "source": [
    "model = LSTMBaselineAtn(n_features=17, n_hidden=50, time_steps=30)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "epochs = 20\n",
    "\n",
    "trainer = TrainUtilHC(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f6af5bc1-5e69-437b-8d0f-a3f25a8cd840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch: 1 train loss: 102.100 val loss: 91.346 score: 30705721.000\n",
      "Epoch: 2 train loss: 99.609 val loss: 88.976 score: 19733672.000\n",
      "Epoch: 3 train loss: 96.735 val loss: 85.774 score: 11791595.250\n",
      "Epoch: 4 train loss: 93.061 val loss: 81.243 score: 6189282.750\n",
      "Epoch: 5 train loss: 87.411 val loss: 74.190 score: 2485348.750\n",
      "Epoch: 6 train loss: 77.470 val loss: 61.318 score: 497107.562\n",
      "Epoch: 7 train loss: 65.036 val loss: 49.634 score: 102145.559\n",
      "Epoch: 8 train loss: 54.394 val loss: 40.299 score: 25159.545\n",
      "Epoch: 9 train loss: 46.150 val loss: 33.690 score: 7725.400\n",
      "Epoch: 10 train loss: 40.263 val loss: 29.873 score: 3260.595\n",
      "Epoch: 11 train loss: 36.711 val loss: 27.956 score: 1943.964\n",
      "Epoch: 12 train loss: 34.821 val loss: 26.915 score: 1481.059\n",
      "Epoch: 13 train loss: 33.228 val loss: 26.081 score: 1260.547\n",
      "Epoch: 14 train loss: 32.477 val loss: 25.303 score: 1116.995\n",
      "Epoch: 15 train loss: 31.501 val loss: 24.543 score: 1005.788\n",
      "Epoch: 16 train loss: 30.801 val loss: 23.774 score: 919.359\n",
      "Epoch: 17 train loss: 30.307 val loss: 23.108 score: 849.239\n",
      "Epoch: 18 train loss: 29.954 val loss: 22.589 score: 792.255\n",
      "Epoch: 19 train loss: 29.454 val loss: 22.069 score: 747.569\n",
      "Epoch: 20 train loss: 28.984 val loss: 21.668 score: 711.144\n",
      "Epoch: 21 train loss: 28.559 val loss: 21.284 score: 682.427\n",
      "Epoch: 22 train loss: 28.363 val loss: 20.956 score: 660.239\n",
      "Epoch: 23 train loss: 27.963 val loss: 20.742 score: 641.214\n",
      "Epoch: 24 train loss: 27.932 val loss: 20.548 score: 626.160\n",
      "Epoch: 25 train loss: 27.445 val loss: 20.364 score: 614.513\n",
      "Epoch: 26 train loss: 27.553 val loss: 20.285 score: 601.911\n",
      "Epoch: 27 train loss: 27.254 val loss: 20.137 score: 591.992\n",
      "Epoch: 28 train loss: 27.191 val loss: 20.051 score: 582.498\n",
      "Epoch: 29 train loss: 26.630 val loss: 19.910 score: 574.138\n",
      "Epoch: 30 train loss: 26.682 val loss: 19.644 score: 566.178\n",
      "Epoch: 31 train loss: 26.477 val loss: 19.506 score: 557.097\n",
      "Epoch: 32 train loss: 26.221 val loss: 19.363 score: 548.920\n",
      "Epoch: 33 train loss: 26.185 val loss: 19.255 score: 540.268\n",
      "Epoch: 34 train loss: 25.985 val loss: 19.045 score: 533.289\n",
      "Epoch: 35 train loss: 25.920 val loss: 18.911 score: 526.426\n",
      "Epoch: 36 train loss: 25.728 val loss: 18.694 score: 520.653\n",
      "Epoch: 37 train loss: 25.505 val loss: 18.584 score: 513.470\n",
      "Epoch: 38 train loss: 25.210 val loss: 18.458 score: 507.783\n",
      "Epoch: 39 train loss: 25.024 val loss: 18.322 score: 503.528\n",
      "Epoch: 40 train loss: 24.827 val loss: 18.120 score: 502.328\n"
     ]
    }
   ],
   "source": [
    "model = LSTMHCAtn()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "epochs = 40\n",
    "\n",
    "trainer = TrainUtilHC(model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    max_rul=MAX_RUL,\n",
    "                    verbosity=0)\n",
    "history = trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d3684-2e55-43ed-a1e6-cefedb1fb0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dde800d-c9d2-4c7f-b2ae-de3b504e9c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306be2a-a934-4cd5-9053-f2d113ebb8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef20ff3d-925e-4a77-a16d-f7a300bfde51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):   \n",
    "    def __init__(self, X, y, seq_len=1):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index:index+self.seq_len], self.y[index+self.seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7255a3db-8a64-4592-9d28-1dc9fff65c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        train_df,\n",
    "        test_df,\n",
    "        label_name,\n",
    "        sequence_length,\n",
    "        batch_size,\n",
    "        n_epochs,\n",
    "        n_epochs_stop,\n",
    "        lr\n",
    "):\n",
    "    \"\"\"Train LSTM model.\"\"\"\n",
    "    print(\"Starting with model training...\")\n",
    "\n",
    "    # create dataloaders\n",
    "    train_dataset = TimeSeriesDataset(np.array(train_df), np.array(train_df[label_name]), seq_len=sequence_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_dataset = TimeSeriesDataset(np.array(test_df), np.array(test_df[label_name]), seq_len=sequence_length)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "    # set up training\n",
    "    #n_features = train_df.shape[1]\n",
    "    #model = TSModel(n_features)\n",
    "    criterion = torch.nn.MSELoss()  # L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_hist = []\n",
    "    test_hist = []\n",
    "\n",
    "    # start training\n",
    "    best_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            data = torch.Tensor(np.array(data))\n",
    "            \n",
    "            if (epoch == 1) and (batch_idx == 1):\n",
    "                print(\"input shape:\", *data.shape)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            if (epoch == 1) and (batch_idx == 1):\n",
    "                print(\"model output:\", *output.shape)\n",
    "                print()\n",
    "            \n",
    "            loss = criterion(output.flatten(), target.type_as(output))\n",
    "            # if type(criterion) == torch.nn.modules.loss.MSELoss:\n",
    "            #     loss = torch.sqrt(loss)  # RMSE\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        running_loss /= len(train_loader)\n",
    "        train_hist.append(running_loss)\n",
    "\n",
    "        # test loss\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data = torch.Tensor(np.array(data))\n",
    "                output = model(data)\n",
    "                loss = criterion(output.flatten(), target.type_as(output))\n",
    "                test_loss += loss.item()\n",
    "            test_loss /= len(test_loader)\n",
    "            test_hist.append(test_loss)\n",
    "\n",
    "            # early stopping\n",
    "            if test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                torch.save(model.state_dict(), Path(model_dir, 'model.pt'))\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            if epochs_no_improve == n_epochs_stop:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch} train loss: {round(running_loss,4)} test loss: {round(test_loss,4)}')\n",
    "\n",
    "        hist = pd.DataFrame()\n",
    "        hist['training_loss'] = train_hist\n",
    "        hist['test_loss'] = test_hist\n",
    "\n",
    "    print(\"Completed.\")\n",
    "\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "900da2b7-5b1a-403c-ab46-efc2b3b3eece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High_Low_Pct</th>\n",
       "      <th>Open_Close_Pct</th>\n",
       "      <th>Day_Of_Week</th>\n",
       "      <th>Month_Of_Year</th>\n",
       "      <th>Quarter_Of_Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>0.081764</td>\n",
       "      <td>0.193811</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006946</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>0.047454</td>\n",
       "      <td>0.192291</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009084</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012580</td>\n",
       "      <td>0.023945</td>\n",
       "      <td>0.019690</td>\n",
       "      <td>0.191727</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051411</td>\n",
       "      <td>0.077130</td>\n",
       "      <td>0.239106</td>\n",
       "      <td>0.186352</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.654109</td>\n",
       "      <td>0.044822</td>\n",
       "      <td>0.048717</td>\n",
       "      <td>0.171630</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.650104</td>\n",
       "      <td>0.029176</td>\n",
       "      <td>0.084043</td>\n",
       "      <td>0.159411</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.592014</td>\n",
       "      <td>0.032559</td>\n",
       "      <td>0.141697</td>\n",
       "      <td>0.171933</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.583220</td>\n",
       "      <td>0.036661</td>\n",
       "      <td>0.119429</td>\n",
       "      <td>0.201692</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.626679</td>\n",
       "      <td>0.021036</td>\n",
       "      <td>0.094614</td>\n",
       "      <td>0.184985</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Close    Volume  High_Low_Pct  Open_Close_Pct  Day_Of_Week  \\\n",
       "0    0.000000  0.008925      0.081764        0.193811     0.833333   \n",
       "1    0.006946  0.020057      0.047454        0.192291     1.000000   \n",
       "2    0.009084  0.011096      0.000000        0.189650     0.000000   \n",
       "3    0.012580  0.023945      0.019690        0.191727     0.166667   \n",
       "4    0.051411  0.077130      0.239106        0.186352     0.333333   \n",
       "..        ...       ...           ...             ...          ...   \n",
       "360  0.654109  0.044822      0.048717        0.171630     0.166667   \n",
       "361  0.650104  0.029176      0.084043        0.159411     0.333333   \n",
       "362  0.592014  0.032559      0.141697        0.171933     0.500000   \n",
       "363  0.583220  0.036661      0.119429        0.201692     0.666667   \n",
       "364  0.626679  0.021036      0.094614        0.184985     0.833333   \n",
       "\n",
       "     Month_Of_Year  Quarter_Of_Year  \n",
       "0              1.0              1.0  \n",
       "1              1.0              1.0  \n",
       "2              1.0              1.0  \n",
       "3              1.0              1.0  \n",
       "4              1.0              1.0  \n",
       "..             ...              ...  \n",
       "360            1.0              1.0  \n",
       "361            1.0              1.0  \n",
       "362            1.0              1.0  \n",
       "363            1.0              1.0  \n",
       "364            1.0              1.0  \n",
       "\n",
       "[365 rows x 7 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63c951a8-f8d0-46c4-8e96-a8924696c49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High_Low_Pct</th>\n",
       "      <th>Open_Close_Pct</th>\n",
       "      <th>Day_Of_Week</th>\n",
       "      <th>Month_Of_Year</th>\n",
       "      <th>Quarter_Of_Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.641769</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.063496</td>\n",
       "      <td>0.198277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.572847</td>\n",
       "      <td>0.040276</td>\n",
       "      <td>0.194433</td>\n",
       "      <td>0.172954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.570286</td>\n",
       "      <td>0.047717</td>\n",
       "      <td>0.068952</td>\n",
       "      <td>1.233623</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.617127</td>\n",
       "      <td>0.053446</td>\n",
       "      <td>0.110560</td>\n",
       "      <td>0.191364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.591876</td>\n",
       "      <td>0.025529</td>\n",
       "      <td>0.060670</td>\n",
       "      <td>0.182496</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>-0.040103</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.014960</td>\n",
       "      <td>0.187984</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>-0.032200</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.032581</td>\n",
       "      <td>0.187136</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>-0.034257</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>-0.012056</td>\n",
       "      <td>0.190973</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>-0.034348</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>-0.024171</td>\n",
       "      <td>0.190842</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>-0.034851</td>\n",
       "      <td>-0.014045</td>\n",
       "      <td>-0.015794</td>\n",
       "      <td>0.138219</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Close    Volume  High_Low_Pct  Open_Close_Pct  Day_Of_Week  \\\n",
       "0    0.641769  0.009487      0.063496        0.198277     1.000000   \n",
       "1    0.572847  0.040276      0.194433        0.172954     0.000000   \n",
       "2    0.570286  0.047717      0.068952        1.233623     0.166667   \n",
       "3    0.617127  0.053446      0.110560        0.191364     0.333333   \n",
       "4    0.591876  0.025529      0.060670        0.182496     0.500000   \n",
       "..        ...       ...           ...             ...          ...   \n",
       "360 -0.040103  0.002671      0.014960        0.187984     0.333333   \n",
       "361 -0.032200  0.005144      0.032581        0.187136     0.500000   \n",
       "362 -0.034257  0.004637     -0.012056        0.190973     0.666667   \n",
       "363 -0.034348 -0.018307     -0.024171        0.190842     0.833333   \n",
       "364 -0.034851 -0.014045     -0.015794        0.138219     1.000000   \n",
       "\n",
       "     Month_Of_Year  Quarter_Of_Year  \n",
       "0              1.0              1.0  \n",
       "1              1.0              1.0  \n",
       "2              1.0              1.0  \n",
       "3              1.0              1.0  \n",
       "4              1.0              1.0  \n",
       "..             ...              ...  \n",
       "360            1.0              1.0  \n",
       "361            1.0              1.0  \n",
       "362            1.0              1.0  \n",
       "363            1.0              1.0  \n",
       "364            1.0              1.0  \n",
       "\n",
       "[365 rows x 7 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39decdb3-52c2-4996-a694-9490c4244d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12-12</td>\n",
       "      <td>18051.320313</td>\n",
       "      <td>18919.550781</td>\n",
       "      <td>18046.041016</td>\n",
       "      <td>18803.656250</td>\n",
       "      <td>18803.656250</td>\n",
       "      <td>21752580802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-12-13</td>\n",
       "      <td>18806.765625</td>\n",
       "      <td>19381.535156</td>\n",
       "      <td>18734.332031</td>\n",
       "      <td>19142.382813</td>\n",
       "      <td>19142.382813</td>\n",
       "      <td>25450468637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-14</td>\n",
       "      <td>19144.492188</td>\n",
       "      <td>19305.099609</td>\n",
       "      <td>19012.708984</td>\n",
       "      <td>19246.644531</td>\n",
       "      <td>19246.644531</td>\n",
       "      <td>22473997681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-15</td>\n",
       "      <td>19246.919922</td>\n",
       "      <td>19525.007813</td>\n",
       "      <td>19079.841797</td>\n",
       "      <td>19417.076172</td>\n",
       "      <td>19417.076172</td>\n",
       "      <td>26741982541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>19418.818359</td>\n",
       "      <td>21458.908203</td>\n",
       "      <td>19298.316406</td>\n",
       "      <td>21310.597656</td>\n",
       "      <td>21310.597656</td>\n",
       "      <td>44409011479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>2022-12-08</td>\n",
       "      <td>16847.349609</td>\n",
       "      <td>17267.916016</td>\n",
       "      <td>16788.783203</td>\n",
       "      <td>17233.474609</td>\n",
       "      <td>17233.474609</td>\n",
       "      <td>20496603770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>2022-12-09</td>\n",
       "      <td>17232.148438</td>\n",
       "      <td>17280.546875</td>\n",
       "      <td>17100.835938</td>\n",
       "      <td>17133.152344</td>\n",
       "      <td>17133.152344</td>\n",
       "      <td>20328426366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>2022-12-10</td>\n",
       "      <td>17134.220703</td>\n",
       "      <td>17216.826172</td>\n",
       "      <td>17120.683594</td>\n",
       "      <td>17128.724609</td>\n",
       "      <td>17128.724609</td>\n",
       "      <td>12706781969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>2022-12-11</td>\n",
       "      <td>17129.710938</td>\n",
       "      <td>17245.634766</td>\n",
       "      <td>17091.820313</td>\n",
       "      <td>17104.193359</td>\n",
       "      <td>17104.193359</td>\n",
       "      <td>14122486832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>2022-12-12</td>\n",
       "      <td>17072.505859</td>\n",
       "      <td>17072.505859</td>\n",
       "      <td>16899.478516</td>\n",
       "      <td>16973.279297</td>\n",
       "      <td>16973.279297</td>\n",
       "      <td>18718484480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>731 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date          Open          High           Low         Close  \\\n",
       "0    2020-12-12  18051.320313  18919.550781  18046.041016  18803.656250   \n",
       "1    2020-12-13  18806.765625  19381.535156  18734.332031  19142.382813   \n",
       "2    2020-12-14  19144.492188  19305.099609  19012.708984  19246.644531   \n",
       "3    2020-12-15  19246.919922  19525.007813  19079.841797  19417.076172   \n",
       "4    2020-12-16  19418.818359  21458.908203  19298.316406  21310.597656   \n",
       "..          ...           ...           ...           ...           ...   \n",
       "726  2022-12-08  16847.349609  17267.916016  16788.783203  17233.474609   \n",
       "727  2022-12-09  17232.148438  17280.546875  17100.835938  17133.152344   \n",
       "728  2022-12-10  17134.220703  17216.826172  17120.683594  17128.724609   \n",
       "729  2022-12-11  17129.710938  17245.634766  17091.820313  17104.193359   \n",
       "730  2022-12-12  17072.505859  17072.505859  16899.478516  16973.279297   \n",
       "\n",
       "        Adj Close       Volume  \n",
       "0    18803.656250  21752580802  \n",
       "1    19142.382813  25450468637  \n",
       "2    19246.644531  22473997681  \n",
       "3    19417.076172  26741982541  \n",
       "4    21310.597656  44409011479  \n",
       "..            ...          ...  \n",
       "726  17233.474609  20496603770  \n",
       "727  17133.152344  20328426366  \n",
       "728  17128.724609  12706781969  \n",
       "729  17104.193359  14122486832  \n",
       "730  16973.279297  18718484480  \n",
       "\n",
       "[731 rows x 7 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/BTC-USD.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8beed2af-5c24-49c6-8ad7-0a29cef8e534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "668ea13f-557b-4143-b046-b8205cc2ab97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with model training...\n",
      "input shape: 10 30 7\n",
      "output shape: 10 30 64 hidden shape: 2 10 64\n",
      "model output: 10 1\n",
      "\n",
      "Epoch 1 train loss: 0.1495 test loss: 0.2826\n",
      "Epoch 2 train loss: 0.0552 test loss: 0.1772\n",
      "Epoch 3 train loss: 0.0611 test loss: 0.1177\n",
      "Epoch 4 train loss: 0.0655 test loss: 0.1097\n",
      "Epoch 5 train loss: 0.0517 test loss: 0.1274\n",
      "Epoch 6 train loss: 0.046 test loss: 0.1184\n",
      "Epoch 7 train loss: 0.0396 test loss: 0.1027\n",
      "Epoch 8 train loss: 0.0378 test loss: 0.0871\n",
      "Epoch 9 train loss: 0.0323 test loss: 0.0674\n",
      "Epoch 10 train loss: 0.0285 test loss: 0.04\n",
      "Epoch 11 train loss: 0.0244 test loss: 0.0245\n",
      "Epoch 12 train loss: 0.011 test loss: 0.0196\n",
      "Epoch 13 train loss: 0.0099 test loss: 0.0512\n",
      "Epoch 14 train loss: 0.0113 test loss: 0.0423\n",
      "Epoch 15 train loss: 0.006 test loss: 0.0314\n",
      "Epoch 16 train loss: 0.0057 test loss: 0.0288\n",
      "Epoch 17 train loss: 0.0055 test loss: 0.026\n",
      "Epoch 18 train loss: 0.0057 test loss: 0.022\n",
      "Epoch 19 train loss: 0.0053 test loss: 0.029\n",
      "Epoch 20 train loss: 0.0054 test loss: 0.0209\n",
      "Epoch 21 train loss: 0.0066 test loss: 0.0276\n",
      "Epoch 22 train loss: 0.0057 test loss: 0.0187\n",
      "Epoch 23 train loss: 0.0076 test loss: 0.0148\n",
      "Epoch 24 train loss: 0.0064 test loss: 0.0134\n",
      "Epoch 25 train loss: 0.0064 test loss: 0.0074\n",
      "Epoch 26 train loss: 0.0058 test loss: 0.0087\n",
      "Epoch 27 train loss: 0.0059 test loss: 0.0038\n",
      "Epoch 28 train loss: 0.0043 test loss: 0.0022\n",
      "Epoch 29 train loss: 0.0038 test loss: 0.0034\n",
      "Epoch 30 train loss: 0.0046 test loss: 0.0029\n",
      "Epoch 31 train loss: 0.0043 test loss: 0.0069\n",
      "Epoch 32 train loss: 0.0041 test loss: 0.0039\n",
      "Epoch 33 train loss: 0.004 test loss: 0.0058\n",
      "Epoch 34 train loss: 0.0038 test loss: 0.0048\n",
      "Epoch 35 train loss: 0.004 test loss: 0.0092\n",
      "Epoch 36 train loss: 0.0042 test loss: 0.0091\n",
      "Epoch 37 train loss: 0.0038 test loss: 0.008\n",
      "Epoch 38 train loss: 0.0038 test loss: 0.0069\n",
      "Epoch 39 train loss: 0.0036 test loss: 0.013\n",
      "Epoch 40 train loss: 0.0041 test loss: 0.0071\n",
      "Completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_loss</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.149510</td>\n",
       "      <td>0.282596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.055157</td>\n",
       "      <td>0.177239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061057</td>\n",
       "      <td>0.117689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.065541</td>\n",
       "      <td>0.109684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051653</td>\n",
       "      <td>0.127381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.045953</td>\n",
       "      <td>0.118389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.039573</td>\n",
       "      <td>0.102711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.037757</td>\n",
       "      <td>0.087150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.032257</td>\n",
       "      <td>0.067441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.028499</td>\n",
       "      <td>0.040018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.024376</td>\n",
       "      <td>0.024465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.010983</td>\n",
       "      <td>0.019571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.009861</td>\n",
       "      <td>0.051168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.042267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005986</td>\n",
       "      <td>0.031440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.005677</td>\n",
       "      <td>0.028820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.005456</td>\n",
       "      <td>0.025988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.005702</td>\n",
       "      <td>0.022026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.029029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.020896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.006589</td>\n",
       "      <td>0.027613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005724</td>\n",
       "      <td>0.018687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.007591</td>\n",
       "      <td>0.014827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.006359</td>\n",
       "      <td>0.013406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.006438</td>\n",
       "      <td>0.007429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.005798</td>\n",
       "      <td>0.008737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.005861</td>\n",
       "      <td>0.003849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.002233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.003431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.004557</td>\n",
       "      <td>0.002862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.006892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.004144</td>\n",
       "      <td>0.003939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.004042</td>\n",
       "      <td>0.005791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.004752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.009177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.004207</td>\n",
       "      <td>0.009082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.007966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.003782</td>\n",
       "      <td>0.006944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.013031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.007065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    training_loss  test_loss\n",
       "0        0.149510   0.282596\n",
       "1        0.055157   0.177239\n",
       "2        0.061057   0.117689\n",
       "3        0.065541   0.109684\n",
       "4        0.051653   0.127381\n",
       "5        0.045953   0.118389\n",
       "6        0.039573   0.102711\n",
       "7        0.037757   0.087150\n",
       "8        0.032257   0.067441\n",
       "9        0.028499   0.040018\n",
       "10       0.024376   0.024465\n",
       "11       0.010983   0.019571\n",
       "12       0.009861   0.051168\n",
       "13       0.011270   0.042267\n",
       "14       0.005986   0.031440\n",
       "15       0.005677   0.028820\n",
       "16       0.005456   0.025988\n",
       "17       0.005702   0.022026\n",
       "18       0.005346   0.029029\n",
       "19       0.005376   0.020896\n",
       "20       0.006589   0.027613\n",
       "21       0.005724   0.018687\n",
       "22       0.007591   0.014827\n",
       "23       0.006359   0.013406\n",
       "24       0.006438   0.007429\n",
       "25       0.005798   0.008737\n",
       "26       0.005861   0.003849\n",
       "27       0.004252   0.002233\n",
       "28       0.003816   0.003431\n",
       "29       0.004557   0.002862\n",
       "30       0.004303   0.006892\n",
       "31       0.004144   0.003939\n",
       "32       0.004042   0.005791\n",
       "33       0.003763   0.004752\n",
       "34       0.003959   0.009177\n",
       "35       0.004207   0.009082\n",
       "36       0.003774   0.007966\n",
       "37       0.003782   0.006944\n",
       "38       0.003597   0.013031\n",
       "39       0.004058   0.007065"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(\n",
    "        TSModel(df_train.shape[1]),\n",
    "        df_train,\n",
    "        df_test,\n",
    "        \"Close\",\n",
    "        sequence_length=30,\n",
    "        batch_size=10,\n",
    "        n_epochs=40,\n",
    "        n_epochs_stop=20,\n",
    "        lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bbe6204b-2470-457c-b15d-3bd90d6cbf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, time_steps=30):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(time_steps, time_steps),\n",
    "            #nn.Softmax(dim=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # inputs: batch size * window size(time step) * lstm output dims\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.permute(0, 2, 1)\n",
    "        x = self.linear(x)\n",
    "        x_probs = x.permute(0, 2, 1)\n",
    "        #print(\"probs\")\n",
    "        #print(x_probs)\n",
    "        #print()\n",
    "        # print(torch.sum(x_probs.item()))\n",
    "        output = x_probs * inputs\n",
    "        return output\n",
    "\n",
    "    \n",
    "class TSModelAttention(nn.Module):\n",
    "    def __init__(self, n_features, time_steps, n_hidden=64, n_layers=2):\n",
    "        super(TSModelAttention, self).__init__()\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.time_steps = time_steps\n",
    "        self.n_features = n_features\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_hidden,\n",
    "            batch_first=True,\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        self.attention = AttentionBlock(time_steps)\n",
    "        self.linear = nn.Linear(self.time_steps*self.n_hidden, 1)\n",
    "        self.printed = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, (hidden, _) = self.lstm(x)\n",
    "        if not self.printed:\n",
    "            print(\"output shape:\", *lstm_output.shape, \"hidden shape:\", *hidden.shape)\n",
    "            self.printed=True\n",
    "            print()\n",
    "        x = self.attention(lstm_output)\n",
    "        #print(x)\n",
    "        #print()\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1, self.time_steps*self.n_hidden)\n",
    "        #print(\"x shape\", x.shape)\n",
    "        lstm_out = hidden[-1]  # output last hidden state output\n",
    "        y_pred = self.linear(x)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5689af84-0be5-4f35-800b-37db9bff1e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with model training...\n",
      "input shape: 10 30 7\n",
      "output shape: 10 30 64 hidden shape: 1 10 64\n",
      "\n",
      "model output: 10 1\n",
      "\n",
      "Epoch 1 train loss: 0.137 test loss: 0.1445\n",
      "Epoch 2 train loss: 0.0605 test loss: 0.1355\n",
      "Epoch 3 train loss: 0.0612 test loss: 0.1225\n",
      "Epoch 4 train loss: 0.0504 test loss: 0.1176\n",
      "Epoch 5 train loss: 0.0432 test loss: 0.1059\n",
      "Epoch 6 train loss: 0.0393 test loss: 0.0937\n",
      "Epoch 7 train loss: 0.036 test loss: 0.0824\n",
      "Epoch 8 train loss: 0.032 test loss: 0.0712\n",
      "Epoch 9 train loss: 0.0274 test loss: 0.0597\n",
      "Epoch 10 train loss: 0.0226 test loss: 0.0484\n",
      "Epoch 11 train loss: 0.0178 test loss: 0.0376\n",
      "Epoch 12 train loss: 0.0134 test loss: 0.0284\n",
      "Epoch 13 train loss: 0.0101 test loss: 0.0243\n",
      "Epoch 14 train loss: 0.0082 test loss: 0.0267\n",
      "Epoch 15 train loss: 0.0076 test loss: 0.0312\n",
      "Epoch 16 train loss: 0.0073 test loss: 0.0344\n",
      "Epoch 17 train loss: 0.0071 test loss: 0.0353\n",
      "Epoch 18 train loss: 0.0073 test loss: 0.035\n",
      "Epoch 19 train loss: 0.0081 test loss: 0.0338\n",
      "Epoch 20 train loss: 0.0094 test loss: 0.0303\n",
      "Epoch 21 train loss: 0.0105 test loss: 0.0226\n",
      "Epoch 22 train loss: 0.0104 test loss: 0.0135\n",
      "Epoch 23 train loss: 0.0092 test loss: 0.0054\n",
      "Epoch 24 train loss: 0.0071 test loss: 0.0024\n",
      "Epoch 25 train loss: 0.0052 test loss: 0.0067\n",
      "Epoch 26 train loss: 0.0054 test loss: 0.0154\n",
      "Epoch 27 train loss: 0.006 test loss: 0.0252\n",
      "Epoch 28 train loss: 0.0061 test loss: 0.0334\n",
      "Epoch 29 train loss: 0.0056 test loss: 0.0336\n",
      "Epoch 30 train loss: 0.0055 test loss: 0.0315\n",
      "Completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_loss</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.137022</td>\n",
       "      <td>0.144499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060537</td>\n",
       "      <td>0.135549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.122494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050381</td>\n",
       "      <td>0.117582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.043236</td>\n",
       "      <td>0.105892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.039307</td>\n",
       "      <td>0.093682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.082431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.032012</td>\n",
       "      <td>0.071198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.027418</td>\n",
       "      <td>0.059735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.022584</td>\n",
       "      <td>0.048439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.017793</td>\n",
       "      <td>0.037609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.013441</td>\n",
       "      <td>0.028420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.024293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.008158</td>\n",
       "      <td>0.026747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.007568</td>\n",
       "      <td>0.031176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.007269</td>\n",
       "      <td>0.034354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.007065</td>\n",
       "      <td>0.035335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.007263</td>\n",
       "      <td>0.034953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.008113</td>\n",
       "      <td>0.033783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.009412</td>\n",
       "      <td>0.030267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.010514</td>\n",
       "      <td>0.022622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.010398</td>\n",
       "      <td>0.013545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.009201</td>\n",
       "      <td>0.005398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.002446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.005416</td>\n",
       "      <td>0.015384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.025241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.006053</td>\n",
       "      <td>0.033447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.033606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.031540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    training_loss  test_loss\n",
       "0        0.137022   0.144499\n",
       "1        0.060537   0.135549\n",
       "2        0.061200   0.122494\n",
       "3        0.050381   0.117582\n",
       "4        0.043236   0.105892\n",
       "5        0.039307   0.093682\n",
       "6        0.035971   0.082431\n",
       "7        0.032012   0.071198\n",
       "8        0.027418   0.059735\n",
       "9        0.022584   0.048439\n",
       "10       0.017793   0.037609\n",
       "11       0.013441   0.028420\n",
       "12       0.010116   0.024293\n",
       "13       0.008158   0.026747\n",
       "14       0.007568   0.031176\n",
       "15       0.007269   0.034354\n",
       "16       0.007065   0.035335\n",
       "17       0.007263   0.034953\n",
       "18       0.008113   0.033783\n",
       "19       0.009412   0.030267\n",
       "20       0.010514   0.022622\n",
       "21       0.010398   0.013545\n",
       "22       0.009201   0.005398\n",
       "23       0.007060   0.002446\n",
       "24       0.005216   0.006700\n",
       "25       0.005416   0.015384\n",
       "26       0.006025   0.025241\n",
       "27       0.006053   0.033447\n",
       "28       0.005581   0.033606\n",
       "29       0.005466   0.031540"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(\n",
    "        TSModelAttention(df_train.shape[1],  30, n_hidden=64, n_layers=1),\n",
    "        df_train,\n",
    "        df_test,\n",
    "        \"Close\",\n",
    "        sequence_length=30,\n",
    "        batch_size=10,\n",
    "        n_epochs=30,\n",
    "        n_epochs_stop=20,\n",
    "        lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572dcec5-e1e2-4782-9304-41870506ce00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_dl] *",
   "language": "python",
   "name": "conda-env-ml_dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
